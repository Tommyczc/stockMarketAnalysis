{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 合并表格"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ### 读取\n",
    "# trd_dalyr0=pd.read_excel(\"stockData/furtherInformation/日个股回报率文件185043337(仅供中央财经大学使用)/TRD_Dalyr.xlsx\")\n",
    "# trd_dalyr0=trd_dalyr0.drop([0,1],axis=0)\n",
    "# trd_dalyr1=pd.read_excel(\"stockData/furtherInformation/日个股回报率文件185043337(仅供中央财经大学使用)/TRD_Dalyr1.xlsx\")\n",
    "# trd_dalyr1=trd_dalyr1.drop([0,1],axis=0)\n",
    "# trd_dalyr2=pd.read_excel(\"stockData/furtherInformation/日个股回报率文件185043337(仅供中央财经大学使用)/TRD_Dalyr2.xlsx\")\n",
    "# trd_dalyr2=trd_dalyr2.drop([0,1],axis=0)\n",
    "# trd_dalyr3=pd.read_excel(\"stockData/furtherInformation/日个股回报率文件185043337(仅供中央财经大学使用)/TRD_Dalyr3.xlsx\")\n",
    "# trd_dalyr3=trd_dalyr3.drop([0,1],axis=0)\n",
    "# trd_dalyr4=pd.read_excel(\"stockData/furtherInformation/日个股回报率文件185043337(仅供中央财经大学使用)/TRD_Dalyr4.xlsx\")\n",
    "# trd_dalyr4=trd_dalyr4.drop([0,1],axis=0)\n",
    "# diaryData=[trd_dalyr0,trd_dalyr1,trd_dalyr2,trd_dalyr3,trd_dalyr4]\n",
    "# finalDF=pd.concat(diaryData)\n",
    "# finalDF"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.819451700Z",
     "start_time": "2023-08-28T14:05:06.304909100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# companyData=pd.read_excel(\"stockData/furtherInformation/公司文件184301535(仅供中央财经大学使用)/TRD_Co.xlsx\")\n",
    "# companyData=companyData.drop([0,1],axis=0)\n",
    "# companyData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.835409300Z",
     "start_time": "2023-08-28T14:05:06.821446800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# companyAssetData=pd.read_excel(\"stockData/furtherInformation/资产负债表(联表查询)181028530/FS_Combas(Merge Query).xlsx\")\n",
    "# companyAssetData=companyAssetData.drop([0,1],axis=0)\n",
    "# companyAssetData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.861346600Z",
     "start_time": "2023-08-28T14:05:06.835409300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# ### 去除b部分\n",
    "# companyAssetData=companyAssetData[companyAssetData[\"FS_Combas.Typrep\"]!=\"B\"]\n",
    "# companyAssetData=companyAssetData.reindex()\n",
    "# ### 去除12-31，因为与1-1有重复\n",
    "# companyAssetData=companyAssetData[companyAssetData['FS_Combas.Accper'].str.contains(\"01-01|03-31|06-30|09-30\")]\n",
    "# companyAssetData['FS_Combas.Accper']=pd.to_datetime(companyAssetData['FS_Combas.Accper'], format=\"%Y-%m-%d\")\n",
    "# \n",
    "# companyAssetData=companyAssetData.rename(columns={\"FS_Combas.Stkcd\":\"Stkcd\",\"FS_Combas.Accper\":\"Trddt_quarter\"})\n",
    "# companyAssetData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.874312200Z",
     "start_time": "2023-08-28T14:05:06.852370400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# ### todo：开始合并 日个股回报与公司信息\n",
    "# df=pd.merge(finalDF,companyData,on='Stkcd')\n",
    "# df['Trddt']=pd.to_datetime(df[\"Trddt\"], format=\"%Y-%m-%d\")\n",
    "# df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.889272400Z",
     "start_time": "2023-08-28T14:05:06.867331700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "###转换时间为季度\n",
    "def dateToQuarter(date):\n",
    "        # print(\"time: \",date)\n",
    "        quarter_date={1:\"01-01\",2:\"03-31\",3:\"06-30\",4:\"09-30\"}\n",
    "        year=date.astype('datetime64[Y]').astype(int) + 1970\n",
    "        month = date.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "        the_quarter = (month - 1) // 3 + 1                  #计算季度\n",
    "        quarter_name = str(year) + '年' + str(the_quarter) + '季度'\n",
    "        quarter_name_short = str(year) + 'Q' + str(the_quarter)\n",
    "        # print(\"quarter time: \",quarter_name)\n",
    "        return [str(year)+\"-\"+quarter_date[the_quarter],the_quarter]\n",
    "\n",
    "# dateToQuarter(numpy.datetime64(\"2020-04-30\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.908268600Z",
     "start_time": "2023-08-28T14:05:06.884285800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# df[\"Trddt_quarter\"]=[dateToQuarter(i)[0] for i in df['Trddt'].values]\n",
    "# df[\"Trddt_quarter\"]=pd.to_datetime(df[\"Trddt_quarter\"], format=\"%Y-%m-%d\")\n",
    "# df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.911238300Z",
     "start_time": "2023-08-28T14:05:06.898256400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# ###todo：根据季度与股票代码合并之前的总表与资产负债表\n",
    "# allData=pd.merge(df,companyAssetData,on=[\"Stkcd\",'Trddt_quarter'],how='outer')\n",
    "# # allData['Stkcd']=allData['Stkcd'].astype(str)\n",
    "# allData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.934160300Z",
     "start_time": "2023-08-28T14:05:06.913216800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# allData.to_csv(\"stockData/furtherInformation/AllData.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.964119300Z",
     "start_time": "2023-08-28T14:05:06.927178700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据去重，并且删除部分缺失值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# allData=pd.read_csv(\"stockData/furtherInformation/AllData.csv\",index_col=0,dtype=object)\n",
    "# allData['Trddt']=pd.to_datetime(allData[\"Trddt\"], format=\"%Y-%m-%d\")\n",
    "# allData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.965115300Z",
     "start_time": "2023-08-28T14:05:06.946128500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# ###todo：要删除的重复列名\n",
    "# deleteColumns=[\"Trddt_quarter\",\"FS_Combas.Typrep\",\"FS_Combas.ShortName\",\"csmar_listedcoinfo.Stknme\",\"csmar_listedcoinfo.Conme\",\"FS_Comscfd.ShortName\",\"FS_Comins.ShortName\",\"csmar_listedcoinfo.Conme_en\",\"csmar_listedcoinfo.Nnindcd\",\"csmar_listedcoinfo.PROVINCECD\",\"csmar_listedcoinfo.CITY\",\"csmar_listedcoinfo.Nnindnme\"]\n",
    "# afterDeleteData=allData[[i for i in allData.columns if i not in deleteColumns]]\n",
    "# afterDeleteData"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:06.979040100Z",
     "start_time": "2023-08-28T14:05:06.960120400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# afterDeleteData.to_csv(\"stockData/furtherInformation/afterDeleteData.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:07.002010600Z",
     "start_time": "2023-08-28T14:05:06.975057700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# ### 检查是否有缺失值达到50%的，有的话就删除\n",
    "# totalNumRows=5000412\n",
    "# num = afterDeleteData.isna().sum()\n",
    "# indexs=[]\n",
    "# nullNums=[]\n",
    "# nullPerc=[]\n",
    "# for index in num.index:\n",
    "#     indexs.append(index)\n",
    "#     nullNums.append(num[index])\n",
    "#     nullPerc.append(str(round(float(num[index]/totalNumRows),3)*100)+\" %\")\n",
    "# \n",
    "# num=pd.DataFrame.from_dict({\"Variance\":indexs,\"Number Of Null\":nullNums,\"Percentage Of Null\":nullPerc})\n",
    "# num"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:07.014978Z",
     "start_time": "2023-08-28T14:05:06.992007200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# ###todo：删除和处理回归不能输入的数据,删除回报率为null的行,同时删除缺失值过多的 [FS_Combas.A001127000,FS_Combas.A001220000,FS_Combas.A001219000,FS_Combas.A001109000] 列\n",
    "# uselessColumns=[\"FS_Combas.A001127000\",\"FS_Combas.A001220000\",\"FS_Combas.A001219000\",\"FS_Combas.A001109000\",\"Stknme\",\"Listdt\",\"csmar_listedcoinfo.BUSSINESSRANGE\",\"Conme_en\",\"PROVINCECODE\",\"Parvcur\",\"csmar_listedcoinfo.MAINBUSSINESS\",\"CITYCODE\",\"PROVINCE\",\"Cuntrycd\",\"Conme\",\"Nindcd\",\"Nindcd\",\"Nnindcd\",\"OWNERSHIPTYPECODE\",\"Opnprc\",\"Hiprc\",\"Loprc\",\"Clsprc\",\"Indcd\",\"csmar_listedcoinfo.OWNERSHIPTYPE\",\"Markettype\",\"Sctcd\"]\n",
    "# \n",
    "# uselessDeletedDf=allData[[i for i in allData.columns if i not in deleteColumns and i not in uselessColumns]]\n",
    "# uselessDeletedDf=uselessDeletedDf.dropna()\n",
    "# uselessDeletedDf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:07.026912300Z",
     "start_time": "2023-08-28T14:05:07.006966100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "###查看并确定目前的数据类型\n",
    "# print(\"占有类型： \",uselessDeletedDf[\"OWNERSHIPTYPE\"].unique())\n",
    "# print(\"财产类型：\",uselessDeletedDf[\"csmar_listedcoinfo.EquityNature\"].unique())\n",
    "# print(\"产业类型：\",uselessDeletedDf[\"Indnme\"].unique())\n",
    "# print(\"产业细分1：\",uselessDeletedDf[\"Nindnme\"].unique())\n",
    "# print(\"产业细分2：\",uselessDeletedDf[\"Nnindnme\"].unique())\n",
    "# print(\"公司所在地区：\",uselessDeletedDf[\"CITY\"].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:07.045901300Z",
     "start_time": "2023-08-28T14:05:07.019931100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# uselessDeletedDf.to_csv(\"stockData/furtherInformation/Input.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:07.057829900Z",
     "start_time": "2023-08-28T14:05:07.034922400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据转换 与 回归"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "uselessDeletedDf=pd.read_csv(\"stockData/furtherInformation/Input.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:05:27.216923300Z",
     "start_time": "2023-08-28T14:05:07.051846200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\87066\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 使用onehot 与 lable转换一些数据\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder\n",
    "uselessDeletedDf['Trddt']=pd.to_datetime(uselessDeletedDf[\"Trddt\"], format=\"%Y-%m-%d\")\n",
    "start = datetime(2019, 1, 1)\n",
    "end = datetime(2020, 1, 1)\n",
    "uselessDeletedDf=uselessDeletedDf[(uselessDeletedDf['Trddt'] < end) | (uselessDeletedDf['Trddt'] > start)]\n",
    "###增加一个季度变量\n",
    "lableEncoder=LabelEncoder()\n",
    "uselessDeletedDf['Quarter']=[dateToQuarter(i)[1] for i in uselessDeletedDf['Trddt'].values]\n",
    "Trddt_lable=lableEncoder.fit_transform([[x] for x in uselessDeletedDf['Trddt'].values])\n",
    "uselessDeletedDf['Trddt']=Trddt_lable\n",
    "# Stkcd_lable=lableEncoder.fit_transform([[x] for x in uselessDeletedDf['Stkcd'].values])\n",
    "# uselessDeletedDf['Stkcd']=Trddt_lable\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "OWNERSHIPTYPE_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['OWNERSHIPTYPE'].values])\n",
    "EquityNature_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['csmar_listedcoinfo.EquityNature'].values])\n",
    "Indnme_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['Indnme'].values])\n",
    "Nindnme_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['Nindnme'].values])\n",
    "Nnindnme_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['Nnindnme'].values])\n",
    "CITY_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['CITY'].values])\n",
    "# uselessDeletedDf['Stkcd']=uselessDeletedDf['Stkcd'].astype(int)\n",
    "# Stkcd_onehot = encoder.fit_transform([[x] for x in uselessDeletedDf['Stkcd'].values])\n",
    "# Stkcd_onehot"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:07:42.017306900Z",
     "start_time": "2023-08-28T14:05:27.221909200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def addColumnToDataFrame(name:str, theArray, theDataFrame=uselessDeletedDf):\n",
    "    theDict={}\n",
    "    thelist = [[r[col] for r in theArray] for col in range(len(theArray[0]))]\n",
    "    # print(f\"{name} column: {len(thelist)}\")\n",
    "    # print(f\"{name} row: {[len(a) for a in Color_onehot][0]}\")\n",
    "    numOfColumn = 0\n",
    "    for column in thelist:\n",
    "        theDict[name + str(numOfColumn)] = column\n",
    "        numOfColumn += 1\n",
    "    newDF=pd.DataFrame.from_dict(theDict)\n",
    "    theDataFrame=pd.concat([theDataFrame,newDF])\n",
    "    # return theDataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:07:42.079141400Z",
     "start_time": "2023-08-28T14:07:42.067173600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.84 GiB for an array with shape (69, 7476204) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m addColumnToDataFrame(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsmar_listedcoinfo.EquityNature\u001B[39m\u001B[38;5;124m'\u001B[39m,EquityNature_onehot)\n\u001B[0;32m      3\u001B[0m addColumnToDataFrame(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mIndnme\u001B[39m\u001B[38;5;124m'\u001B[39m,Indnme_onehot)\n\u001B[1;32m----> 4\u001B[0m \u001B[43maddColumnToDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mNindnme\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mNindnme_onehot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m addColumnToDataFrame(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNnindnme\u001B[39m\u001B[38;5;124m'\u001B[39m,Nnindnme_onehot)\n\u001B[0;32m      6\u001B[0m addColumnToDataFrame(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCITY\u001B[39m\u001B[38;5;124m'\u001B[39m,CITY_onehot)\n",
      "Cell \u001B[1;32mIn[19], line 11\u001B[0m, in \u001B[0;36maddColumnToDataFrame\u001B[1;34m(name, theArray, theDataFrame)\u001B[0m\n\u001B[0;32m      9\u001B[0m     numOfColumn \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     10\u001B[0m newDF\u001B[38;5;241m=\u001B[39mpd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_dict(theDict)\n\u001B[1;32m---> 11\u001B[0m theDataFrame\u001B[38;5;241m=\u001B[39m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtheDataFrame\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnewDF\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Apps\\anaconda\\envs\\DataAnalysis\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001B[0m, in \u001B[0;36mconcat\u001B[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[0m\n\u001B[0;32m    370\u001B[0m     copy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    372\u001B[0m op \u001B[38;5;241m=\u001B[39m _Concatenator(\n\u001B[0;32m    373\u001B[0m     objs,\n\u001B[0;32m    374\u001B[0m     axis\u001B[38;5;241m=\u001B[39maxis,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    382\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[0;32m    383\u001B[0m )\n\u001B[1;32m--> 385\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Apps\\anaconda\\envs\\DataAnalysis\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001B[0m, in \u001B[0;36m_Concatenator.get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    612\u001B[0m             indexers[ax] \u001B[38;5;241m=\u001B[39m obj_labels\u001B[38;5;241m.\u001B[39mget_indexer(new_labels)\n\u001B[0;32m    614\u001B[0m     mgrs_indexers\u001B[38;5;241m.\u001B[39mappend((obj\u001B[38;5;241m.\u001B[39m_mgr, indexers))\n\u001B[1;32m--> 616\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mconcatenate_managers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    617\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcat_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbm_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\n\u001B[0;32m    618\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write():\n\u001B[0;32m    620\u001B[0m     new_data\u001B[38;5;241m.\u001B[39m_consolidate_inplace()\n",
      "File \u001B[1;32mE:\\Apps\\anaconda\\envs\\DataAnalysis\\lib\\site-packages\\pandas\\core\\internals\\concat.py:242\u001B[0m, in \u001B[0;36mconcatenate_managers\u001B[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001B[0m\n\u001B[0;32m    240\u001B[0m     fastpath \u001B[38;5;241m=\u001B[39m blk\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m values\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m    241\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 242\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[43m_concatenate_join_units\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjoin_units\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m     fastpath \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fastpath:\n",
      "File \u001B[1;32mE:\\Apps\\anaconda\\envs\\DataAnalysis\\lib\\site-packages\\pandas\\core\\internals\\concat.py:613\u001B[0m, in \u001B[0;36m_concatenate_join_units\u001B[1;34m(join_units, copy)\u001B[0m\n\u001B[0;32m    610\u001B[0m     concat_values \u001B[38;5;241m=\u001B[39m ensure_block_shape(concat_values, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    612\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 613\u001B[0m     concat_values \u001B[38;5;241m=\u001B[39m \u001B[43mconcat_compat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mto_concat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    615\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat_values\n",
      "File \u001B[1;32mE:\\Apps\\anaconda\\envs\\DataAnalysis\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py:117\u001B[0m, in \u001B[0;36mconcat_compat\u001B[1;34m(to_concat, axis, ea_compat_axis)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     target_dtype \u001B[38;5;241m=\u001B[39m np_find_common_type(\u001B[38;5;241m*\u001B[39mdtypes)\n\u001B[1;32m--> 117\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mto_concat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kinds \u001B[38;5;129;01mand\u001B[39;00m result\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;66;03m# GH#39817 cast to object instead of casting bools to numeric\u001B[39;00m\n\u001B[0;32m    120\u001B[0m     result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 3.84 GiB for an array with shape (69, 7476204) and data type float64"
     ]
    }
   ],
   "source": [
    "addColumnToDataFrame('OWNERSHIPTYPE',OWNERSHIPTYPE_onehot)\n",
    "addColumnToDataFrame('csmar_listedcoinfo.EquityNature',EquityNature_onehot)\n",
    "addColumnToDataFrame('Indnme',Indnme_onehot)\n",
    "addColumnToDataFrame('Nindnme',Nindnme_onehot)\n",
    "addColumnToDataFrame('Nnindnme',Nnindnme_onehot)\n",
    "addColumnToDataFrame('CITY',CITY_onehot)\n",
    "# addColumnToDataFrame('Stkcd',Stkcd_onehot)\n",
    "uselessDeletedDf=uselessDeletedDf.drop(columns=['OWNERSHIPTYPE','csmar_listedcoinfo.EquityNature','Indnme','Nindnme','Nnindnme','CITY','Stkcd'])\n",
    "uselessDeletedDf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:11:52.020912900Z",
     "start_time": "2023-08-28T14:07:42.073158300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### train & test data\n",
    "Y=uselessDeletedDf['ChangeRatio']\n",
    "X=uselessDeletedDf.drop(columns='ChangeRatio')\n",
    "x_train , x_test , y_train , y_test = train_test_split(X,Y , test_size= 0.3 ,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-28T14:11:52.000966400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-28T14:11:52.076285600Z",
     "start_time": "2023-08-28T14:11:52.051311800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = linear_regressor.predict(x_test)\n",
    "y_trainPredicted=linear_regressor.predict(x_train[-3000:])\n",
    "LR = pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\n",
    "LR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-28T14:11:52.051311800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(LR[:50])\n",
    "plt.legend(['Actual' , 'Predicted'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-28T14:11:52.056506100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
