{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.0 Get the raw news information\n",
    "* 本篇代码用来获取相关公司的新闻文本\n",
    "* 由于信息渠道广泛，将会爬虫数个新闻网站: [新浪新闻，微信公众号]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 新浪新闻获取"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, news\n",
      "Load time  0:00:00.223654\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import xlwt as xlwt\n",
    "\n",
    "print(\"Hello, news\")\n",
    "from datetime import datetime\n",
    "now=datetime.now()\n",
    "import numpy as np\n",
    "from pylab import mpl\n",
    "# todo: solve chinese problem for plt\n",
    "mpl.rcParams['font.sans-serif']=['SimHei']\n",
    "mpl.rcParams['axes.unicode_minus']=False\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json\n",
    "from pyppeteer import launch\n",
    "from bs4 import BeautifulSoup\n",
    "from pyppeteer import launcher\n",
    "launcher.DEFAULT_ARGS.remove(\"--enable-automation\")\n",
    "print(\"Load time \",datetime.now()-now)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.026247Z",
     "start_time": "2023-11-17T04:07:53.774987Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from requests import RequestException\n",
    "\n",
    "\n",
    "class newsToolForSina:\n",
    "    @staticmethod\n",
    "    def get_list(url):\n",
    "\n",
    "        # 新闻链接\n",
    "        res=requests.get(url)\n",
    "        res.encoding='utf-8'\n",
    "\n",
    "        # 完整HTML\n",
    "        html=BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        # 新闻列表\n",
    "        newList=[]\n",
    "\n",
    "        for item in html.find_all('div',['news-item','img-news-item']):\n",
    "            try:\n",
    "                newObj={}\n",
    "                newObj['title']=item.select('h2 a')[0].text\n",
    "                newObj['url']=item.select('h2 a')[0].get('href')\n",
    "                newList.append(newObj)\n",
    "            except:\n",
    "                print('出现异常')\n",
    "        return newList\n",
    "\n",
    "    @staticmethod\n",
    "    def get_HtmlDetail_Sina(url):\n",
    "\n",
    "        # 新闻链接\n",
    "        res=requests.get(url)\n",
    "        res.encoding='utf-8'\n",
    "\n",
    "        # 完整HTML\n",
    "        html=BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "        # 新闻对象\n",
    "        result={}\n",
    "\n",
    "        # 新闻标题\n",
    "        result['title']=html.select('.main-title')[0].text\n",
    "\n",
    "        # 发布时间\n",
    "        timesource=html.select('.date-source span')[0].text\n",
    "        createtime=datetime.strptime(timesource,'%Y年%m月%d日 %H:%M')\n",
    "        createtime.strftime('%Y-%m-%d')\n",
    "        result['createtime']=createtime\n",
    "\n",
    "        # 新闻来源\n",
    "        result['place']=html.select('.date-source a')[0].text\n",
    "\n",
    "        # 新闻内容\n",
    "        article=[]\n",
    "        for p in html.select('#article p')[:-1]:\n",
    "            article.append(p.text.strip())\n",
    "        articleText=' '.join(article)\n",
    "        result['article']=articleText\n",
    "\n",
    "        # 新闻作者\n",
    "        result['author']=html.select('.show_author')[0].text.strip('责任编辑：')\n",
    "\n",
    "        # 新闻链接\n",
    "        result['url']=url\n",
    "\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_page_news(browser,urlList):\n",
    "        #获取当前页面所有包含新闻的a标签\n",
    "        news = browser.find_elements(\"xpath\",'//div[@class=\"d_list_txt\"]/ul/li/span/a')\n",
    "        if len(news)==0:\n",
    "            return np.nan\n",
    "        for i in news:\n",
    "            link = i.get_attribute('href') #得到新闻url\n",
    "            # print(len(urlList),link)\n",
    "            if link not in urlList:  #通过url去重\n",
    "                urlList.append(link)\n",
    "        return urlList\n",
    "\n",
    "    @staticmethod\n",
    "    def getNewsData(url):\n",
    "        # 获取新闻的详细信息\n",
    "        html = newsToolForSina.get_response(url)\n",
    "        #使用beautifulsoup进行解析\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "        #标题\n",
    "        '''\n",
    "        <h1 class=\"main-title\">证监会要求北京银行说明是否串通*ST康得管理层舞弊</h1>\n",
    "        '''\n",
    "        title = soup.select('.main-title')\n",
    "        #可能有小部分标题的标签不是上述格式 对其进行补充\n",
    "        if not title:\n",
    "            title = soup.select('#artibodyTitle')\n",
    "        if title:\n",
    "            title = title[0].text\n",
    "        # print(title)\n",
    "\n",
    "        #日期\n",
    "        '''\n",
    "        <span class=\"date\">2019年07月20日 16:52</span>\n",
    "        '''\n",
    "        date = soup.select('.date')\n",
    "        # 可能有小部分日期的标签不是上述格式 对其进行补充\n",
    "        if not date:\n",
    "            date = soup.select('#pub_date')\n",
    "        if date:\n",
    "            date = date[0].text\n",
    "        # print(date)\n",
    "\n",
    "        #来源\n",
    "        '''\n",
    "        <span class=\"source ent-source\">中国证券报</span>\n",
    "        '''\n",
    "        source = soup.select('.source')\n",
    "        # 可能有小部分来源的标签不是上述格式 对其进行补充\n",
    "        if not source:\n",
    "            source = soup.select('[data-sudaclick=\"media_name\"]')\n",
    "        if source:\n",
    "            source = source[0].text\n",
    "        # print(source)\n",
    "\n",
    "        #正文\n",
    "        article = soup.select('div[class=\"article\"] p')\n",
    "        # 可能有小部分正文的标签不是上述格式 对其进行补充\n",
    "        if not article:\n",
    "            article = soup.select('div[id=\"artibody\"] p')\n",
    "        if article:\n",
    "            #把正文放在一个列表中 每个p标签的内容为列表的一项\n",
    "            article_list = []\n",
    "            for i in article:\n",
    "                # print(i.text)\n",
    "                article_list.append(i.text)\n",
    "        #转为字典格式\n",
    "        news = {'link': url, 'title': title, 'date': date, 'source': source, 'article': article_list}\n",
    "        return news\n",
    "\n",
    "    @staticmethod\n",
    "    def get_response(url):\n",
    "        try:\n",
    "            #添加User-Agent，放在headers中，伪装成浏览器\n",
    "            headers = {\n",
    "                'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url,headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                response.encoding = 'utf-8'\n",
    "                return response.text\n",
    "            return None\n",
    "        except RequestException:\n",
    "            return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.036006Z",
     "start_time": "2023-11-17T04:07:54.026974Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "###test\n",
    "# newList=newsToolForSina.get_list('https://news.sina.com.cn/world/')\n",
    "# resultList=pd.DataFrame()\n",
    "#\n",
    "# for i,item in enumerate(newList):\n",
    "#     try:\n",
    "#         result=newsToolForSina.get_HtmlDetail_Sina(item['url'])\n",
    "#         newObj=pd.DataFrame(result,index=[0])\n",
    "#         resultList=resultList.append(newObj)\n",
    "#         print (str(i),'写入成功')\n",
    "#     except BaseException as err:\n",
    "#         print (str(i),'出现异常: ',err)\n",
    "#\n",
    "# resultList.to_csv(\"newsData/test.csv\",header=resultList.columns)\n",
    "# resultList"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.036147Z",
     "start_time": "2023-11-17T04:07:54.030377Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 注意: 新浪新闻的历史数据只能查到近期一个月的，并不能查之前的，所以新浪的可以放弃一部分了"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# from selenium.common import NoSuchElementException\n",
    "#\n",
    "# #打开浏览器\n",
    "# UrlList=[]\n",
    "# browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "# browser.implicitly_wait(10)\n",
    "# #打开网址\n",
    "# browser.get('https://news.sina.com.cn/roll/')\n",
    "# #获取当前页面新闻的url\n",
    "# UrlList=newsToolForSina.get_page_news(browser,urlList=UrlList)\n",
    "# while True:\n",
    "#     try:\n",
    "#         #找到下一页按钮 并点击\n",
    "#         '''\n",
    "#         <a href=\"javascript:void(0)\" onclick=\"newsList.page.next();return false;\">下一页</a>\n",
    "#         '''\n",
    "#         browser.find_element(\"xpath\",'//a[@onclick=\"newsList.page.next();return false;\"]').click()\n",
    "#         #获取下一页新闻的url\n",
    "#         result=newsToolForSina.get_page_news(browser,urlList=UrlList)\n",
    "#         if result is np.nan:\n",
    "#             break\n",
    "#     except BaseException as err:\n",
    "#         print(err)\n",
    "#         browser.close()\n",
    "#\n",
    "#\n",
    "# print(\"Have found {} URL records, trying to get news text data\".format(len(UrlList)))\n",
    "# resultList=pd.DataFrame()\n",
    "# for i,item in enumerate(UrlList):\n",
    "#     try:\n",
    "#         result=newsToolForSina.getNewsData(item)\n",
    "#         newObj=pd.DataFrame(result)\n",
    "#         resultList=resultList.append(newObj)\n",
    "#         print (str(i),'写入成功')\n",
    "#         # break\n",
    "#     except BaseException as err:\n",
    "#         print (str(i),'出现异常: ',err)\n",
    "#         # break\n",
    "#\n",
    "#\n",
    "# resultList.to_csv(\"newsData/SinaNewsRawData.csv\",header=resultList.columns)\n",
    "# resultList"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.037430Z",
     "start_time": "2023-11-17T04:07:54.033912Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0                                               link  \\\n0               0  https://finance.sina.com.cn/roll/2023-05-12/do...   \n1               1  https://finance.sina.com.cn/roll/2023-05-12/do...   \n2               2  https://finance.sina.com.cn/roll/2023-05-12/do...   \n3               3  https://finance.sina.com.cn/roll/2023-05-12/do...   \n4               4  https://finance.sina.com.cn/roll/2023-05-12/do...   \n...           ...                                                ...   \n47350           2  https://finance.sina.com.cn/tob/2023-05-10/doc...   \n47351           0  https://finance.sina.com.cn/tob/2023-05-10/doc...   \n47352           1  https://finance.sina.com.cn/tob/2023-05-10/doc...   \n47353           0  https://finance.sina.com.cn/tob/2023-05-10/doc...   \n47354           1  https://finance.sina.com.cn/tob/2023-05-10/doc...   \n\n                                 title               date source  \\\n0                 4月居民存款锐减之谜，1.2万亿去哪了？  2023年05月12日 16:48   第一财经   \n1                 4月居民存款锐减之谜，1.2万亿去哪了？  2023年05月12日 16:48   第一财经   \n2                 4月居民存款锐减之谜，1.2万亿去哪了？  2023年05月12日 16:48   第一财经   \n3                 4月居民存款锐减之谜，1.2万亿去哪了？  2023年05月12日 16:48   第一财经   \n4                 4月居民存款锐减之谜，1.2万亿去哪了？  2023年05月12日 16:48   第一财经   \n...                                ...                ...    ...   \n47350      蒙牛乳业5月9日斥资约1826.08万港元回购55万股  2023年05月10日 08:59   新浪港股   \n47351   太古股份公司A5月9日斥资272.95万港元回购4.55万股  2023年05月10日 08:58   新浪港股   \n47352   太古股份公司A5月9日斥资272.95万港元回购4.55万股  2023年05月10日 08:58   新浪港股   \n47353  太古股份公司B5月9日耗资约550.67万港元回购55.5万股  2023年05月10日 08:58   新浪港股   \n47354  太古股份公司B5月9日耗资约550.67万港元回购55.5万股  2023年05月10日 08:58   新浪港股   \n\n                                                 article  \n0                 　　炒股就看金麒麟分析师研报，权威，专业，及时，全面，助您挖掘潜力主题机会！  \n1      　　但这在数据上并未得到印证。国家统计局公布的数据显示，4月份，全国CPI同比上涨0.1%；...  \n2                                 　　4月居民存款大幅减少引起市场的广泛热议。  \n3      　　央行最新公布的4月金融数据显示，4月人民币存款减少4609亿元，同比多减5524亿元，其...  \n4      　　不少分析认为，4月居民存款减少或源自“提前还贷”和消费回升所致；另外的观点则认为，4月份...  \n...                                                  ...  \n47350                                          责任编辑：卢昱君   \n47351  　　太古股份公司A（00019）发布公告，于2023年5月9日斥资272.95万港元回购4....  \n47352                                          责任编辑：卢昱君   \n47353  　　太古股份公司B（00087）公布，2023年5月9日耗资约550.67万港元回购55.5...  \n47354                                          责任编辑：卢昱君   \n\n[47355 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>link</th>\n      <th>title</th>\n      <th>date</th>\n      <th>source</th>\n      <th>article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>https://finance.sina.com.cn/roll/2023-05-12/do...</td>\n      <td>4月居民存款锐减之谜，1.2万亿去哪了？</td>\n      <td>2023年05月12日 16:48</td>\n      <td>第一财经</td>\n      <td>炒股就看金麒麟分析师研报，权威，专业，及时，全面，助您挖掘潜力主题机会！</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>https://finance.sina.com.cn/roll/2023-05-12/do...</td>\n      <td>4月居民存款锐减之谜，1.2万亿去哪了？</td>\n      <td>2023年05月12日 16:48</td>\n      <td>第一财经</td>\n      <td>但这在数据上并未得到印证。国家统计局公布的数据显示，4月份，全国CPI同比上涨0.1%；...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>https://finance.sina.com.cn/roll/2023-05-12/do...</td>\n      <td>4月居民存款锐减之谜，1.2万亿去哪了？</td>\n      <td>2023年05月12日 16:48</td>\n      <td>第一财经</td>\n      <td>4月居民存款大幅减少引起市场的广泛热议。</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>https://finance.sina.com.cn/roll/2023-05-12/do...</td>\n      <td>4月居民存款锐减之谜，1.2万亿去哪了？</td>\n      <td>2023年05月12日 16:48</td>\n      <td>第一财经</td>\n      <td>央行最新公布的4月金融数据显示，4月人民币存款减少4609亿元，同比多减5524亿元，其...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>https://finance.sina.com.cn/roll/2023-05-12/do...</td>\n      <td>4月居民存款锐减之谜，1.2万亿去哪了？</td>\n      <td>2023年05月12日 16:48</td>\n      <td>第一财经</td>\n      <td>不少分析认为，4月居民存款减少或源自“提前还贷”和消费回升所致；另外的观点则认为，4月份...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47350</th>\n      <td>2</td>\n      <td>https://finance.sina.com.cn/tob/2023-05-10/doc...</td>\n      <td>蒙牛乳业5月9日斥资约1826.08万港元回购55万股</td>\n      <td>2023年05月10日 08:59</td>\n      <td>新浪港股</td>\n      <td>责任编辑：卢昱君</td>\n    </tr>\n    <tr>\n      <th>47351</th>\n      <td>0</td>\n      <td>https://finance.sina.com.cn/tob/2023-05-10/doc...</td>\n      <td>太古股份公司A5月9日斥资272.95万港元回购4.55万股</td>\n      <td>2023年05月10日 08:58</td>\n      <td>新浪港股</td>\n      <td>太古股份公司A（00019）发布公告，于2023年5月9日斥资272.95万港元回购4....</td>\n    </tr>\n    <tr>\n      <th>47352</th>\n      <td>1</td>\n      <td>https://finance.sina.com.cn/tob/2023-05-10/doc...</td>\n      <td>太古股份公司A5月9日斥资272.95万港元回购4.55万股</td>\n      <td>2023年05月10日 08:58</td>\n      <td>新浪港股</td>\n      <td>责任编辑：卢昱君</td>\n    </tr>\n    <tr>\n      <th>47353</th>\n      <td>0</td>\n      <td>https://finance.sina.com.cn/tob/2023-05-10/doc...</td>\n      <td>太古股份公司B5月9日耗资约550.67万港元回购55.5万股</td>\n      <td>2023年05月10日 08:58</td>\n      <td>新浪港股</td>\n      <td>太古股份公司B（00087）公布，2023年5月9日耗资约550.67万港元回购55.5...</td>\n    </tr>\n    <tr>\n      <th>47354</th>\n      <td>1</td>\n      <td>https://finance.sina.com.cn/tob/2023-05-10/doc...</td>\n      <td>太古股份公司B5月9日耗资约550.67万港元回购55.5万股</td>\n      <td>2023年05月10日 08:58</td>\n      <td>新浪港股</td>\n      <td>责任编辑：卢昱君</td>\n    </tr>\n  </tbody>\n</table>\n<p>47355 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultList=pd.read_csv(\"newsData/SinaNewsRawData.csv\")\n",
    "resultList"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.321543Z",
     "start_time": "2023-11-17T04:07:54.037206Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.324620Z",
     "start_time": "2023-11-17T04:07:54.321428Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 新闻联播文本稿"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.326989Z",
     "start_time": "2023-11-17T04:07:54.324065Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 微博基于关键字内容爬取 (主要内容: 文章文本，观看人数，评论，点赞)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "import urllib\n",
    "from pyquery import PyQuery as pq\n",
    "\n",
    "search='疫情'###搜索内容\n",
    "dateStart=\"2019-1-1\"   #日期格式必须是：形如：2022-4-21\n",
    "dateEnd=\"2023-11-1\"\n",
    "\n",
    "urlsearch=urllib.parse.quote('=1&q='+search)     #进行url编码\n",
    "m_referer='https://m.weibo.cn/search?containerid=100103type'   #微博搜索来源界面\n",
    "base_url = 'https://m.weibo.cn/api/container/getIndex?'     #微博接口\n",
    "#host用于指定internet主机和端口号，http1.1必须包含，不然系统返回400，\n",
    "headers = {                 #封装请求头  让网站识别自己是浏览器\n",
    "    'Referer': m_referer+urlsearch,             #告诉服务器自己是哪里来的  从那个页面来的   来路\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36 Edg/80.0.361.111',    #包含操作系统和浏览器信息\n",
    "    'Host':'m.weibo.cn',                #请求服务器的域名和端口号\n",
    "    'X-Requested-With':'XMLHttpRequest'             #代表是ajax请求\n",
    "}\n",
    "sign=0      #标志是否找到了实时微博以开始\n",
    "Pqove=0     #标志位  表示是否读取文件结束\n",
    "\n",
    "#获取网页的json（这个是获取搜索之后网页的json数据的函数）\n",
    "def get_page(page):\n",
    "    para={\n",
    "        'containerid':m_referer[m_referer.find('100103'):]+urlsearch,\n",
    "        'page':page\n",
    "    }\n",
    "    url = base_url+urlencode(para)                  #进行url编码添加到地址结尾  连带页数\n",
    "    # print(url)\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)       #request请求  地址和携带请求头\n",
    "        if response.status_code == 200:\n",
    "            return response.json()                              #以json格式返回数据\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"Error:\",e.args)\n",
    "\n",
    "#获取全文网页的json（这个是获取需要对全文获取的网页的json数据的函数）\n",
    "def get_txt_page(containerid,referer):\n",
    "    base_urlx = 'https://m.weibo.cn/statuses/show?'     #相较于获取搜索结果的url不通\n",
    "    txtheaders = {\n",
    "        'Referer': referer,\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36 Edg/80.0.361.111',\n",
    "        'Host': 'm.weibo.cn',\n",
    "        'X-Requested-With': 'XMLHttpRequest'\n",
    "    }\n",
    "    para = {\n",
    "        'id': containerid,    #只需要一个参数  且参数名为id\n",
    "    }\n",
    "    url = base_urlx + urlencode(para)  # 进行url编码添加到地址结尾  连带页数\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)  # request请求  地址和携带请求头\n",
    "        if response.status_code == 200:\n",
    "            return response.json()  # 以json格式返回数据\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"Error:\", e.args)\n",
    "\n",
    "#分析JSON格式的数据，抓取目标信息\n",
    "def parse_json(data):\n",
    "    global sign                         #sign是用来标记是否有过title   在格式中从实时微博进行爬取  找到实时微博title之后开始爬取   之后不用进行判断title而用sign来判断\n",
    "    if data:\n",
    "        items = data.get('data').get('cards')\n",
    "        for item1 in items:\n",
    "            item=item1.get(\"title\")\n",
    "            if(item!=None or sign==1):       #分析数据发现只有实时微博开始时有一个title  所以可以进行判断是否有title并从title开始爬取\n",
    "                item2=item1.get(\"card_group\")\n",
    "                # print(item2)\n",
    "                # print(len(item2))\n",
    "                for card in item2:\n",
    "                    if card.get(\"mblog\")!=None:             #如果没有mblog  那么就结束本次循环不尽兴数据爬取   如果有就进行爬取（因为会有一些数据中不包含mblog代表其并不是所需的数据）\n",
    "                        sign=1\n",
    "                        itemc=card.get(\"mblog\")\n",
    "                        mid=itemc.get(\"mid\")\n",
    "                        weibo = {}\n",
    "                        \n",
    "                        # 抓取信息\n",
    "                        scheme=str(card.get(\"scheme\"))              #获取具体博文链接（可以访问具体博文数据）\n",
    "                        txtwb=pq(itemc.get('text')).text() #获取微博博文\n",
    "                        \n",
    "                        if  txtwb.find(\"全文\")+2==len(txtwb):         #利用微博博文进行判断是否结尾有“全文二字”\n",
    "\n",
    "                            datatxt = get_txt_page(scheme[scheme.find('mblogid=')+8:scheme.find('mblogid=')+17],scheme)  # 如果有全文那么需要进入其中再次进行爬取全文数据，首先切割链接获取mblogid，scheme对应的是Referer的链接信息\n",
    "\n",
    "                            txtitem=pq(datatxt.get('data').get('text')).text()\n",
    "\n",
    "                            weibo['text'] = pq(datatxt.get('data').get('text')).text()          #用pyquery去处理得到的数据\n",
    "                        else:\n",
    "                            weibo['text'] = [pq(itemc.get('text')).text()]      #不需要对全文做处理直接获取text\n",
    "                        weibo['date'] = [str(datetime.strptime(pq(itemc.get('created_at')).text(), '%a %b %d %H:%M:%S +0800 %Y'))]#日期   #修改日期格式   默认微博日期格式是带时区的GMT的格式\n",
    "                        weibo['attitudes'] = [itemc.get('attitudes_count')]  # 点赞次数\n",
    "                        weibo['comments'] = [itemc.get('comments_count')]  # 评论次数\n",
    "                        weibo['reposts'] = [itemc.get('reposts_count')]  # 转发次数\n",
    "                        weibo['userid']=[itemc.get('user').get('id')]   #发布人id\n",
    "                        weibo['username'] = [itemc.get('user').get('screen_name')]  # 发布人微博名\n",
    "                        weibo[\"midid\"]=[mid] ###该微博本身的 id\n",
    "                        # 一个一个返回weibo\n",
    "                        yield weibo\n",
    "            else:\n",
    "\n",
    "                continue\n",
    "                \n",
    "col = [\"发布人id\", \"发布人微博名\", \"发布时间\", \"点赞次数\", \"评论次数\", \"转发次数\", \"博文内容\",\"博文id\"]\n",
    "datalist=[\"userid\", \"username\", \"date\", \"attitudes\", \"comments\", \"reposts\", \"text\",\"midid\"]\n",
    "allWeiboResults=pd.DataFrame()\n",
    "# def transferResultToDF(result:dict,finalResult=allWeiboResults):\n",
    "#     resultDf=pd.DataFrame.from_dict(result)\n",
    "#     finalResult=pd.concat([resultDf,finalResult])\n",
    "#     return finalResult\n",
    "\n",
    "def weibo_main():\n",
    "    global Pqove\n",
    "    bio = 0                 #bio 是用来标记是否连续多个超出规定时间段的博文（连续超过3个就认为博文已经爬取完毕）\n",
    "    page = 1                #页数  从第一页开始  也是需要传入的一个参数\n",
    "    while True:\n",
    "        data = get_page(page)                   #获取网页的json格式的数据\n",
    "        results = parse_json(data)              #解析网页的json数据\n",
    "        for result in results:                  #循环去便利数据\n",
    "            # print(type(result['date']))\n",
    "            d1 = datetime.strptime(result['date'], '%Y-%m-%d %H:%M:%S') #博文时间\n",
    "            d2 = datetime.strptime(dateEnd+' 00:00:00', '%Y-%m-%d %H:%M:%S')           #结束时间\n",
    "            d3 = datetime.strptime(dateStart+' 00:00:00', '%Y-%m-%d %H:%M:%S')          #开始时间\n",
    "            # print(result)\n",
    "            # print(\"-----------\")\n",
    "            # print(d1)\n",
    "            # print(d1<d2)\n",
    "            if d1 >= d3:\n",
    "                                   #逻辑：  判断时间是否在开始和结束时间之间 如果在那么就存入数据   如果不在修改bio的数据   page++   如果bio达到3那么跳出while\n",
    "                if d1 <= d2:\n",
    "                    bio = 0\n",
    "                    ###todo: test\n",
    "                    print(result)\n",
    "                    print(\"-------------------------\")\n",
    "                    #save_data_toexcel(result,savepath)\n",
    "                    # save_date_tolist(result,savepath)\n",
    "                    # save_data(result, page)  # 判断是否存入（要求必须是规定时间之内的微博）   判定逻辑：如果出现连续三个时间超过的规定范围那么便停止抓取\n",
    "                else:\n",
    "                    print(\"该新闻不符合时间要求(太晚了)，已省略\")\n",
    "                    print(\"-------------------------\")\n",
    "                    bio=0\n",
    "            else:\n",
    "                print(\"该新闻不符合时间要求(太早了)，已省略\")\n",
    "                print(\"-------------------------\")\n",
    "                bio += 1\n",
    "                if bio>=10:\n",
    "                    Pqove=1\n",
    "                    #save_data_toexcel(result,savepath)\n",
    "                    ###todo: test\n",
    "                    print(result)\n",
    "                    print(\"-------------------------\")\n",
    "                    break\n",
    "                    \n",
    "        print('第' + str(page) + '页抓取完成')\n",
    "        # print(bio)\n",
    "        if bio>=10:\n",
    "            break\n",
    "        page += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:54.390230Z",
     "start_time": "2023-11-17T04:07:54.345062Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text                 date  \\\n0    名为新冠疫情的那场旷世灾难开始的冬天 遇见一群疗愈人心的可爱鬼//@阿甲kk:天哪已经四年了吗  2023-11-17 11:54:35   \n0  所以有疫情的时候没有感冒 什么都好好的是因为 一直不出门，出门口罩不离嘴 下周开始我要把口罩...  2023-11-17 11:56:16   \n0  疫情之后，鲁汶大学代表团首次访问中国。本周一，北京校友会在北京比利时驻华大使官邸举办。该代表...  2023-11-17 11:58:33   \n0  去年疫情拆东墙补西墙，欠了很多外债，今年本想着好好还账，结果今年又新建了一堵墙，好难啊，什么...  2023-11-17 11:59:47   \n0  疫情时期，西方媒体疯狂报道解放军进武汉，想借机引起市民恐慌，但他们不知道，从唐山到汶川到玉树...  2023-11-17 12:00:00   \n0  【携程集团公共事务部总经理秦静：没有丰富的旅游资源，也可能成为网红城市】11月17日上午，2...  2023-11-17 12:02:42   \n0                             社区医院居然到现在还在每天填写疫情消毒记录表  2023-11-17 12:02:48   \n0  #出圈吧大学生# 疫情之后好多年没有这么热闹过了，校运会终于回来了！拒绝做脆皮大学生，动起来...  2023-11-17 12:06:13   \n\n   attitudes  comments  reposts      userid  username             midid  \n0          1         0        0  3975077990    阿swim_  4969033105672173  \n0          0         0        0  5807389941  早睡不早起哦哦哦  4969033526937368  \n0          3         0        0  1927332783   比利时驻华使馆  4969034103653682  \n0          0         0        0  7111504660      苗苗很苗  4969034413507459  \n0          2         0        0  1455428062       张大曦  4969034469343680  \n0          0         0        0  6452231600     NBD视频  4969035148036266  \n0          0         0        0  3625185894   fodttth  4969035172152676  \n0          0         0        0  2240689720       巫建辉  4969036033557888  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>date</th>\n      <th>attitudes</th>\n      <th>comments</th>\n      <th>reposts</th>\n      <th>userid</th>\n      <th>username</th>\n      <th>midid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>名为新冠疫情的那场旷世灾难开始的冬天 遇见一群疗愈人心的可爱鬼//@阿甲kk:天哪已经四年了吗</td>\n      <td>2023-11-17 11:54:35</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3975077990</td>\n      <td>阿swim_</td>\n      <td>4969033105672173</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>所以有疫情的时候没有感冒 什么都好好的是因为 一直不出门，出门口罩不离嘴 下周开始我要把口罩...</td>\n      <td>2023-11-17 11:56:16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5807389941</td>\n      <td>早睡不早起哦哦哦</td>\n      <td>4969033526937368</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>疫情之后，鲁汶大学代表团首次访问中国。本周一，北京校友会在北京比利时驻华大使官邸举办。该代表...</td>\n      <td>2023-11-17 11:58:33</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1927332783</td>\n      <td>比利时驻华使馆</td>\n      <td>4969034103653682</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>去年疫情拆东墙补西墙，欠了很多外债，今年本想着好好还账，结果今年又新建了一堵墙，好难啊，什么...</td>\n      <td>2023-11-17 11:59:47</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7111504660</td>\n      <td>苗苗很苗</td>\n      <td>4969034413507459</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>疫情时期，西方媒体疯狂报道解放军进武汉，想借机引起市民恐慌，但他们不知道，从唐山到汶川到玉树...</td>\n      <td>2023-11-17 12:00:00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1455428062</td>\n      <td>张大曦</td>\n      <td>4969034469343680</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>【携程集团公共事务部总经理秦静：没有丰富的旅游资源，也可能成为网红城市】11月17日上午，2...</td>\n      <td>2023-11-17 12:02:42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6452231600</td>\n      <td>NBD视频</td>\n      <td>4969035148036266</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>社区医院居然到现在还在每天填写疫情消毒记录表</td>\n      <td>2023-11-17 12:02:48</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3625185894</td>\n      <td>fodttth</td>\n      <td>4969035172152676</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>#出圈吧大学生# 疫情之后好多年没有这么热闹过了，校运会终于回来了！拒绝做脆皮大学生，动起来...</td>\n      <td>2023-11-17 12:06:13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2240689720</td>\n      <td>巫建辉</td>\n      <td>4969036033557888</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###todo test \n",
    "data = get_page(1)                   #获取网页的json格式的数据\n",
    "results = parse_json(data) \n",
    "for result in results:\n",
    "    resultDf=pd.DataFrame.from_dict(result)\n",
    "    allWeiboResults=pd.concat([resultDf,allWeiboResults])\n",
    "    \n",
    "allWeiboResults"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:59.339820Z",
     "start_time": "2023-11-17T04:07:54.361698Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# weibo_main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:59.343044Z",
     "start_time": "2023-11-17T04:07:59.339668Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 微信公众号基于关键字爬取 (主要内容: 文章文本，评论，点赞，收藏)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:59.345216Z",
     "start_time": "2023-11-17T04:07:59.343116Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 卫生办 新冠病毒 人数统计(日)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "class newsToolForHealthCenter:\n",
    "    @staticmethod\n",
    "    def get_page_news(keywords:[],browser,urlList:{},endTime:str):\n",
    "        \n",
    "        news = browser.find_elements(\"xpath\",'/html/body/div[2]/div/div[1]//*[@class=\"wordGuide Residence-permit\"]//*[@class=\"bigTit clearfix\"]/a')\n",
    "        similarArticles=browser.find_elements(\"xpath\",'/html/body/div[2]/div/div[1]//*[@class=\"wordGuide Residence-permit\"]//*[@class=\"listInfoCon clearfix \"]//*[@class=\"listIntro wh100 fl\"]//*[@class=\"similarArticles\"]//*[@class=\"similarArticlesList\"]/ul//*[@class=\"clearfix\"]/a')\n",
    "        if len(news)==0:\n",
    "            return np.nan\n",
    "        print(\"--------------new page--------------\")\n",
    "        if len(similarArticles)>0:\n",
    "            for article in similarArticles:\n",
    "                # print(\"-----------  ------------\")\n",
    "                # print(article.get_attribute(\"text\"))\n",
    "                # print(article.get_attribute(\"href\"))\n",
    "                similartitle=article.get_attribute(\"text\").strip()\n",
    "                similarlink=article.get_attribute(\"href\").strip()\n",
    "                for keyword in keywords:\n",
    "                    if re.match(pattern=keyword,string=similartitle):\n",
    "                        print(\"Found similar article: \",similartitle)\n",
    "                        if similarlink not in urlList.keys():\n",
    "                            urlList[similarlink]=similartitle\n",
    "        for i in news:\n",
    "            title=i.get_attribute(\"title\").strip()\n",
    "            link = i.get_attribute('href').strip() #得到新闻url\n",
    "            for keyword in keywords:\n",
    "                if re.match(pattern=keyword,string=title):\n",
    "                    print(\"Found: \",title)\n",
    "                    if link not in urlList.keys():\n",
    "                        urlList[link]=title\n",
    "        return urlList\n",
    "    \n",
    "    @staticmethod\n",
    "    async def getPageContent(url:str):\n",
    "        publicDate=\"\"\n",
    "        source=\"\"\n",
    "        content=\"\"\n",
    "        dateRe=\"发布时间：.?\"\n",
    "        sourceRe=\"来源:.?\"\n",
    "        # 获取新闻的详细信息\n",
    "        html=await newsToolForHealthCenter.pyppteer_fetchUrl(url)\n",
    "        if html is None:\n",
    "            print(url,\"  is nome type\")\n",
    "            return {\"Public_Date\":np.nan,\"Source\":np.nan,\"Content\":np.nan}\n",
    "        \n",
    "        elif len(html)<=0:\n",
    "            return {\"Public_Date\":np.nan,\"Source\":np.nan,\"Content\":np.nan}\n",
    "        #使用beautifulsoup进行解析\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "\n",
    "        #日期\n",
    "        '''\n",
    "        <source>\n",
    "            <span>发布时间：2022-09-02</span>\n",
    "        '''\n",
    "        origin = soup.find_all('span')\n",
    "        # 可能有小部分日期的标签不是上述格式 对其进行补充\n",
    "        for ele in origin:\n",
    "            if re.match(string=ele.text.strip(),pattern=dateRe):\n",
    "                # print(ele.text)\n",
    "                publicDate=ele.text.replace(\"发布时间：\",\"\").strip()\n",
    "            if re.match(string=ele.text.strip(),pattern=sourceRe):\n",
    "                # print(ele.text)\n",
    "                source=ele.text.replace(\"来源:\",\"\").strip()\n",
    "                \n",
    "        article=\"\"\n",
    "        try: \n",
    "            #正文\n",
    "            article = soup.find(id=\"xw_box\").text.strip()\n",
    "            # print(\"---------------\")\n",
    "            # print(article)\n",
    "            content=article\n",
    "        except BaseException as err:\n",
    "            return {\"Public_Date\":np.nan,\"Source\":np.nan,\"Content\":np.nan}\n",
    "        \n",
    "        return {\"Public_Date\":publicDate,\"Source\":source,\"Content\":content}\n",
    "\n",
    "    @staticmethod\n",
    "    async def pyppteer_fetchUrl(url):\n",
    "        browser = await launch({'headless': False,'dumpio':True, 'autoClose':True})\n",
    "        page = await browser.newPage()\n",
    "        await page.goto(url)\n",
    "        await page.waitFor(100)\n",
    "        await asyncio.wait([page.waitForNavigation(timeout=500000)])\n",
    "        str = await page.content()\n",
    "        await browser.close()\n",
    "        return str\n",
    "    \n",
    "    # @staticmethod\n",
    "    # async def fetchUrl(url):\n",
    "    #     return await newsToolForHealthCenter.pyppteer_fetchUrl(url)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:59.385345Z",
     "start_time": "2023-11-17T04:07:59.345766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "# \n",
    "# # 👇️ call apply()\n",
    "# nest_asyncio.apply()\n",
    "# url = 'http://www.nhc.gov.cn/yjb/s7860/202209/e0a18445e0ab47608527b9c910f77699.shtml'\n",
    "# \n",
    "# async def fetchUrl(url):\n",
    "#     browser = await launch({'headless': False,'dumpio':True, 'autoClose':True})\n",
    "#     page = await browser.newPage()\n",
    "# \n",
    "#     await page.goto(url)\n",
    "#     await asyncio.wait([page.waitForNavigation()])\n",
    "#     str = await page.content()\n",
    "#     await browser.close()\n",
    "#     print(\"-------------------\")\n",
    "#     print(str)\n",
    "# \n",
    "# asyncio.get_event_loop().run_until_complete(fetchUrl(url))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:07:59.398232Z",
     "start_time": "2023-11-17T04:07:59.360411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import wait\n",
    "from selenium.common import NoSuchElementException\n",
    "# \n",
    "#打开浏览器\n",
    "browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "options = Options()\n",
    "# browser = webdriver.Chrome(service=Service(ChromeDriverManager( latest_release_url='https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json', driver_version='116.0.5845.96').install()), options=options)\n",
    "\n",
    "browser.implicitly_wait(100)\n",
    "# #打开网址\n",
    "# # browser.get('http://zs.kaipuyun.cn/s?searchWord=%25E6%2596%25B0%25E5%259E%258B%25E5%2586%25A0%25E7%258A%25B6%25E7%2597%2585%25E6%25AF%2592%25E8%2582%25BA%25E7%2582%258E&column=%25E6%259C%25AC%25E7%25AB%2599&pageSize=10&pageNum=0&siteCode=N000001642&sonSiteCode=&checkHandle=1&searchSource=0&areaSearchFlag=0&secondSearchWords=&topical=&docName=&label=&countKey=0&uc=0&left_right_index=0&searchBoxSettingsIndex=&isSecondSearch=undefined&manualWord=%25E6%2596%25B0%25E5%259E%258B%25E5%2586%25A0%25E7%258A%25B6%25E7%2597%2585%25E6%25AF%2592%25E8%2582%25BA%25E7%2582%258E&orderBy=0&startTime=&endTime=&timeStamp=0&strFileType=&wordPlace=0')\n",
    "# \n",
    "# browser.get('http://zs.kaipuyun.cn/s?searchWord=%25E6%2596%25B0%25E5%259E%258B%25E5%2586%25A0%25E7%258A%25B6%25E7%2597%2585%25E6%25AF%2592%25E8%2582%25BA%25E7%2582%258E%25E7%2596%25AB%25E6%2583%2585%25E6%259C%2580%25E6%2596%25B0%25E6%2583%2585%25E5%2586%25B5&column=%25E6%259C%25AC%25E7%25AB%2599&pageSize=10&pageNum=0&siteCode=N000001642&sonSiteCode=&checkHandle=1&searchSource=1&areaSearchFlag=0&secondSearchWords=&topical=&docName=&label=&countKey=0&uc=0&left_right_index=0&searchBoxSettingsIndex=&isSecondSearch=undefined&manualWord=%25E6%2596%25B0%25E5%259E%258B%25E5%2586%25A0%25E7%258A%25B6%25E7%2597%2585%25E6%25AF%2592%25E8%2582%25BA%25E7%2582%258E%25E7%2596%25AB%25E6%2583%2585%25E6%259C%2580%25E6%2596%25B0%25E6%2583%2585%25E5%2586%25B5&orderBy=0&startTime=&endTime=&timeStamp=0&strFileType=&wordPlace=0')\n",
    "# \n",
    "# #获取当前页面新闻的url\n",
    "# keywords=[\"截至.*新型冠状病毒肺炎疫情最新情况\",'.*新型冠状病毒感染的肺炎疫情情况']\n",
    "# endTime=\"2018-6-6\"\n",
    "# urlList={}\n",
    "# urlList=newsToolForHealthCenter.get_page_news(keywords,browser,urlList=urlList,endTime=endTime)\n",
    "# \n",
    "# while True:\n",
    "#     try:\n",
    "#         #找到下一页按钮 并点击\n",
    "#         browser.find_element(\"xpath\",'//*[@id=\"pageInfo\"]//*[@class=\"next\"]/a').click()\n",
    "#         #获取下一页新闻的url\n",
    "#         result=newsToolForHealthCenter.get_page_news(keywords,browser,urlList=urlList,endTime=endTime)\n",
    "#         if result is np.nan:\n",
    "#             break\n",
    "#     except BaseException as err:\n",
    "#         # print(err)\n",
    "#         browser.close()\n",
    "#         break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.659242Z",
     "start_time": "2023-11-17T04:07:59.363493Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# print(len(urlList))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.667779Z",
     "start_time": "2023-11-17T04:08:08.659777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# ###其中有一个文章链接会报错\n",
    "# allHealthCenterNews=pd.DataFrame()\n",
    "# for link,title in urlList.items():\n",
    "#     contentAndTime=asyncio.run(newsToolForHealthCenter.getPageContent(url=link))\n",
    "#     theNewsInstance={\"Title\":[title],\"Link\":[link],\"Content\":[contentAndTime['Content']],\"Public_Time\":[contentAndTime['Public_Date']],\"Source\":[contentAndTime['Source']]}\n",
    "#     theNewsDf=pd.DataFrame.from_dict(theNewsInstance)\n",
    "#     allHealthCenterNews=pd.concat([theNewsDf,allHealthCenterNews])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.667927Z",
     "start_time": "2023-11-17T04:08:08.663693Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# allHealthCenterNews"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.671432Z",
     "start_time": "2023-11-17T04:08:08.667461Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# allHealthCenterNews.to_csv(\"stockData/furtherInformation/covid19/CovidNews01.csv\",index=False,encoding=\"utf-8\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.678482Z",
     "start_time": "2023-11-17T04:08:08.670860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "     Unnamed: 0                      Title  \\\n0             0   截至12月2日24时新型冠状病毒肺炎疫情最新情况   \n1             1  截至12月12日24时新型冠状病毒肺炎疫情最新情况   \n2             2  截至12月25日24时新型冠状病毒肺炎疫情最新情况   \n3             3  截至12月12日24时新型冠状病毒肺炎疫情最新情况   \n4             4  截至12月25日24时新型冠状病毒肺炎疫情最新情况   \n..          ...                        ...   \n362         362  截至10月22日24时新型冠状病毒肺炎疫情最新情况   \n363         363    截至9月6日24时新型冠状病毒肺炎疫情最新情况   \n364         364   截至9月30日24时新型冠状病毒肺炎疫情最新情况   \n365         365   截至10月4日24时新型冠状病毒肺炎疫情最新情况   \n366         366   截至10月6日24时新型冠状病毒肺炎疫情最新情况   \n\n                                                  Link  \\\n0    http://www.nhc.gov.cn/yjb/s7860/202112/4bfc964...   \n1    http://www.nhc.gov.cn/yjb/s7860/202112/7a5bf7e...   \n2    http://www.nhc.gov.cn/yjb/s7860/202112/f24d106...   \n3    http://www.nhc.gov.cn/yjb/s7860/202112/7a5bf7e...   \n4    http://www.nhc.gov.cn/yjb/s7860/202112/f24d106...   \n..                                                 ...   \n362  http://www.nhc.gov.cn/yjb/s7860/202210/9409772...   \n363  http://www.nhc.gov.cn/yjb/s7860/202209/b9867ea...   \n364  http://www.nhc.gov.cn/yjb/s7860/202210/32fc5f9...   \n365  http://www.nhc.gov.cn/yjb/s7860/202210/20fc250...   \n366  http://www.nhc.gov.cn/yjb/s7860/202210/175086e...   \n\n                                               Content Public_Time   Source  \n0    12月2日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例96例。其...  2021-12-03  卫生应急办公室  \n1    12月12日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例101例...  2021-12-13  卫生应急办公室  \n2    12月25日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例206例...  2021-12-26  卫生应急办公室  \n3    12月12日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例101例...  2021-12-13  卫生应急办公室  \n4    12月25日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例206例...  2021-12-26  卫生应急办公室  \n..                                                 ...         ...      ...  \n362  10月22日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例207例...  2022-10-23  卫生应急办公室  \n363  9月6日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例380例。其...  2022-09-07  卫生应急办公室  \n364  9月30日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例172例。...  2022-10-01  卫生应急办公室  \n365  10月4日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例273例。...  2022-10-05  卫生应急办公室  \n366  10月6日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例288例。...  2022-10-07  卫生应急办公室  \n\n[367 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Link</th>\n      <th>Content</th>\n      <th>Public_Time</th>\n      <th>Source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>截至12月2日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202112/4bfc964...</td>\n      <td>12月2日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例96例。其...</td>\n      <td>2021-12-03</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>截至12月12日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202112/7a5bf7e...</td>\n      <td>12月12日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例101例...</td>\n      <td>2021-12-13</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>截至12月25日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202112/f24d106...</td>\n      <td>12月25日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例206例...</td>\n      <td>2021-12-26</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>截至12月12日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202112/7a5bf7e...</td>\n      <td>12月12日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例101例...</td>\n      <td>2021-12-13</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>截至12月25日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202112/f24d106...</td>\n      <td>12月25日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例206例...</td>\n      <td>2021-12-26</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>362</td>\n      <td>截至10月22日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202210/9409772...</td>\n      <td>10月22日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例207例...</td>\n      <td>2022-10-23</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>363</td>\n      <td>截至9月6日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202209/b9867ea...</td>\n      <td>9月6日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例380例。其...</td>\n      <td>2022-09-07</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>364</td>\n      <td>截至9月30日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202210/32fc5f9...</td>\n      <td>9月30日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例172例。...</td>\n      <td>2022-10-01</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>365</td>\n      <td>截至10月4日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202210/20fc250...</td>\n      <td>10月4日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例273例。...</td>\n      <td>2022-10-05</td>\n      <td>卫生应急办公室</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>366</td>\n      <td>截至10月6日24时新型冠状病毒肺炎疫情最新情况</td>\n      <td>http://www.nhc.gov.cn/yjb/s7860/202210/175086e...</td>\n      <td>10月6日0—24时，31个省（自治区、直辖市）和新疆生产建设兵团报告新增确诊病例288例。...</td>\n      <td>2022-10-07</td>\n      <td>卫生应急办公室</td>\n    </tr>\n  </tbody>\n</table>\n<p>367 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allHealthCenterNews=pd.read_csv(\"stockData/furtherInformation/covid19/CovidNews.csv\")\n",
    "allHealthCenterNews"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.706005Z",
     "start_time": "2023-11-17T04:08:08.674888Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get the city name in china"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# from tkinter import _flatten\n",
    "# \n",
    "# ## url: https://zh.wikipedia.org/zh-hans/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E5%9F%8E%E5%B8%82%E5%88%97%E8%A1%A8\n",
    "# \n",
    "# def filterChara(content:str):\n",
    "#     content=[c.replace(\"地级市：\",\"\").replace(\"县级市：\",\"\").replace(\"副省级市：\",\"\").strip().split(\"、\") for c in content.split(\"\\n\")]\n",
    "#     content=list(_flatten(content))\n",
    "#     return content\n",
    "# \n",
    "# browser.get('https://zh.wikipedia.org/zh-hans/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E5%9F%8E%E5%B8%82%E5%88%97%E8%A1%A8')\n",
    "# \n",
    "# cities = browser.find_elements(\"xpath\",'//*[@class=\"mw-content-container\"]//*[@id=\"content\"]//*[@id=\"bodyContent\"]//*[@id=\"mw-content-text\"]//*[@class=\"mw-parser-output\"]/h3//*[@class=\"mw-headline\"]/a')\n",
    "# \n",
    "# towns = browser.find_elements(\"xpath\",'//*[@class=\"mw-content-container\"]//*[@id=\"content\"]//*[@id=\"bodyContent\"]//*[@id=\"mw-content-text\"]//*[@class=\"mw-parser-output\"]/ul')\n",
    "# \n",
    "# directlyCities=[\"北京市\",\"天津市\",\"上海市\",\"重庆市\"]\n",
    "# specialArea=[\"香港特别行政区\",\"澳门特别行政区\"]\n",
    "# \n",
    "# allCityAndTown={}\n",
    "# \n",
    "# cityList=[\"直辖市\",\"特别行政区\"]\n",
    "# for city in cities:\n",
    "#     # print(\"-------------------------\")\n",
    "#     # print(city.get_attribute(\"title\"))\n",
    "#     cityList.append(city.get_attribute(\"title\"))\n",
    "# \n",
    "# townList=[]\n",
    "# for index,town in enumerate(towns):\n",
    "#     if index>=len(cityList):\n",
    "#         break\n",
    "#     for j,city in enumerate(cityList):\n",
    "#         if index==j:\n",
    "#             # print(city,\":  \\n\",filterChara(town.text))\n",
    "#             allCityAndTown[city]=filterChara(town.text)\n",
    "# allCityAndTown"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.713141Z",
     "start_time": "2023-11-17T04:08:08.708008Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# import json\n",
    "# file = open(\"dictionary_data.json\", \"w\")\n",
    "# json.dump(allCityAndTown, file)\n",
    "# file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T04:08:08.714900Z",
     "start_time": "2023-11-17T04:08:08.711026Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-27T12:05:31.903516Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
