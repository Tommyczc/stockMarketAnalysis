{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.0 Get Data Of Stock Market\n",
    "\n",
    "* 本篇代码用来查询所有股票数据，所有导出的数据都将导出到stockdata文件夹内，方便管理查看。\n",
    "* 平台api方面，国内用tushare，国外用的雅虎。\n",
    "* 本篇代码也计划用来做数据清洗并且数据展示，处理好的文件也存在stockdata文件夹里面。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Hello,stock market\")\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "import time\n",
    "import numpy as np\n",
    "## todo: calculating the time of import libraries\n",
    "#先引入后面分析、可视化等可能用到的库\n",
    "import tushare as ts\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#正常显示画图时出现的中文和负号\n",
    "from pylab import mpl\n",
    "import yfinance as yf\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "# todo: solve chinese problem for plt\n",
    "mpl.rcParams['font.sans-serif']=['SimHei']\n",
    "mpl.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "#显示所有列\n",
    "# pd.set_option('display.max_columns', 100)\n",
    "# #显示所有行\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# #设置value的显示长度为100，默认为50\n",
    "# pd.set_option('max_colwidth',100)\n",
    "print(\"Load Time: \",datetime.now()-start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 use tushare api get data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# login tushare, use token\n",
    "pro = ts.pro_api('f3987990df2a25038e5123dd321b5ed941048135b923e9c6415f20e1')\n",
    "\n",
    "def getStockCompanyInformation(status='L', stock_code=''):\n",
    "    \"\"\"\n",
    "    得到目前所有的股票公司信息\n",
    "    :param status: 上市状态 L上市 D退市 P暂停上市，默认是L\n",
    "    :param stock_code: TS股票代码\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if stock_code== '':\n",
    "        rawStockCompanyDataInTushare = pro.stock_basic(exchange='', list_status=status, fields='ts_code,symbol,fullname,enname,name,area,industry,list_date')\n",
    "    else:\n",
    "        rawStockCompanyDataInTushare = pro.stock_basic(ts_code=stock_code, exchange='', list_status=status, fields='ts_code,symbol,fullname,enname,name,area,industry,list_date')\n",
    "\n",
    "    rawStockCompanyDataInTushare['ts_code']=rawStockCompanyDataInTushare['ts_code'].astype(str)\n",
    "    rawStockCompanyDataInTushare['symbol']=rawStockCompanyDataInTushare['symbol'].astype('float64')\n",
    "    rawStockCompanyDataInTushare['name']=rawStockCompanyDataInTushare['name'].astype(str)\n",
    "    rawStockCompanyDataInTushare['area']=rawStockCompanyDataInTushare['area'].astype(str)\n",
    "    rawStockCompanyDataInTushare['industry']=rawStockCompanyDataInTushare['industry'].astype(str)\n",
    "    rawStockCompanyDataInTushare['fullname']=rawStockCompanyDataInTushare['fullname'].astype(str)\n",
    "    rawStockCompanyDataInTushare['enname']=rawStockCompanyDataInTushare['enname'].astype(str)\n",
    "    rawStockCompanyDataInTushare['list_date']=rawStockCompanyDataInTushare['list_date'].astype('float64')\n",
    "\n",
    "    rawStockCompanyDataInTushare['list_date']=pd.to_datetime(rawStockCompanyDataInTushare['list_date'], format='%Y%m%d')\n",
    "    return rawStockCompanyDataInTushare\n",
    "\n",
    "\n",
    "# 拉取数据\n",
    "def getDailyStockData(stock_code=\"\",start=\"20130101\",end=\"20230101\"):\n",
    "    \"\"\"\n",
    "    拉取股票日结交易信息，时间段默认 2018-1-1 到 2022-1-1，注意：该函数单次只能查询6000条记录。\n",
    "    :param stock_code: 股票代码，默认为空，输入为空的时候，拉取这段时期所有股票的交易消息；若不为空，拉去这只股票这段时间的交易消息\n",
    "    :param start: 开始日期\n",
    "    :param end: 结束日期\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rawData = pro.daily(**{\n",
    "        \"ts_code\": stock_code,\n",
    "        \"trade_date\": \"\",\n",
    "        \"start_date\": start,\n",
    "        \"end_date\": end,\n",
    "        \"offset\": \"\",\n",
    "        \"limit\": \"\",\n",
    "    }, fields=[\n",
    "        \"ts_code\",\n",
    "        \"trade_date\",\n",
    "        \"open\",\n",
    "        \"high\",\n",
    "        \"low\",\n",
    "        \"close\",\n",
    "        \"pre_close\",\n",
    "        \"change\",\n",
    "        \"pct_chg\",\n",
    "        \"vol\",\n",
    "        \"amount\"\n",
    "    ])\n",
    "    rawData['ts_code']=rawData['ts_code'].astype(str)\n",
    "    rawData['trade_date']=rawData['trade_date'].astype('float64')\n",
    "\n",
    "    rawData['trade_date']=pd.to_datetime(rawData['trade_date'], format='%Y%m%d')\n",
    "    return rawData"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rawCompanyDataInTushare=getStockCompanyInformation()\n",
    "# rawCompanyDataInTushare.to_csv('stockData/rawCompanyDataInTushare.csv',header=rawCompanyDataInTushare.columns,index=False)\n",
    "\n",
    "rawCompanyDataInTushare=pd.read_csv('stockData/rawCompanyDataInTushare.csv')\n",
    "rawCompanyDataInTushare['industry'].replace([np.nan,''],'None', inplace=True)\n",
    "\n",
    "print(\"The shape of the data frame: {}\".format(rawCompanyDataInTushare.shape))\n",
    "print(\"The types in the data frame {}\".format(rawCompanyDataInTushare.dtypes))\n",
    "rawCompanyDataInTushare"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rawDailyDataInTushare=getDailyStockData()\n",
    "# print(\"The shape of the data frame: {}\".format(rawDailyDataInTushare.shape))\n",
    "# print(\"The types in the data frame:\\n{}\".format(rawDailyDataInTushare.dtypes))\n",
    "# print(\"The unique stock code: {}\".format(len(rawDailyDataInTushare['ts_code'].unique())))\n",
    "# rawDailyDataInTushare"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##test api\n",
    "# testDailyStockData=getDailyStockData(stock_code=\"000006.SZ\")\n",
    "# testDailyStockData"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###由于Toshare社区限制每次查询只能拿到 6000 行日结交易信息（每日每公司为一行），而且每分钟只能查500次。为了得到当前所有上市公司，每次查询只查一家上市公司而且将会延迟0.2秒\n",
    "def bindDailyAndCompanyInformation():\n",
    "    notIncludedData=['name']\n",
    "\n",
    "    startTime=datetime.now()\n",
    "    companies=getStockCompanyInformation()\n",
    "    bindData=pd.DataFrame()\n",
    "    i=2\n",
    "    for company in companies['ts_code'].to_numpy():\n",
    "        dailyData=getDailyStockData(stock_code=company)\n",
    "        for header in companies.columns:\n",
    "            if header!=\"ts_code\":\n",
    "                #test only\n",
    "                # print(\"------------------------------------\")\n",
    "                # print(companies[companies['ts_code'] == company][header].values.repeat(5))\n",
    "                # print(dailyData.shape[0])\n",
    "                # print(\"------------------------------------\")\n",
    "                dailyData[header]= companies[companies['ts_code'] == company][header].values.repeat(dailyData.shape[0])\n",
    "\n",
    "        dailyData=dailyData.drop(columns=notIncludedData)\n",
    "        bindData=bindData.append(dailyData)\n",
    "        #time.sleep(0.15)\n",
    "        ###########test only\n",
    "        #print(dailyData)\n",
    "        # if i==1:\n",
    "        #     break\n",
    "        # else:\n",
    "        #     i-=1\n",
    "        ###########\n",
    "    timeConsume=datetime.now()-startTime\n",
    "    print(\"Run time: {}\".format(timeConsume))\n",
    "    return bindData\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###此cell仅用于下载股票数据，由于信息量庞大，可导入下好的 TushareRawData.csv 文件即可\n",
    "# theBindDataInTushare=bindDailyAndCompanyInformation()\n",
    "# print(\"The shape of the data frame: {}\".format(theBindDataInTushare.shape))\n",
    "# print(\"The types in the data frame:\\n{}\".format(theBindDataInTushare.dtypes))\n",
    "# print(\"The unique stock code: {}\".format(len(theBindDataInTushare['ts_code'].unique())))\n",
    "# theBindDataInTushare.to_csv('stockData/TushareRawData.csv', header=theBindDataInTushare.columns, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###此cell是本地读取tushare股票信息，如果本地有csv文件就用这个，并且注释上一个cell，如果没下载csv文件就取消注释上一个cell\n",
    "theBindDataInTushare=pd.read_csv('stockData/TushareRawData.csv')\n",
    "print(\"The shape of the data frame: {}\".format(theBindDataInTushare.shape))\n",
    "print(\"The types in the data frame:\\n{}\".format(theBindDataInTushare.dtypes))\n",
    "print(\"The unique stock code: {}\".format(len(theBindDataInTushare['ts_code'].unique())))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 不知道为什么有100家公司查不到，手动查询看看情况，然后手动合并吧\n",
    "missedCompany=[]\n",
    "bindCompany=theBindDataInTushare['ts_code'].unique()\n",
    "for code in rawCompanyDataInTushare['ts_code'].values:\n",
    "    if code not in bindCompany:\n",
    "        missedCompany.append(code)\n",
    "\n",
    "\n",
    "# for code in missedCompany:\n",
    "#     print(\"-------------------------------------\")\n",
    "#     print(getDailyStockData(stock_code=code,end=\"20220505\"))\n",
    "#     print(getStockCompanyInformation(stock_code=code))\n",
    "#     print(\"-------------------------------------\")\n",
    "\n",
    "print(\"The num of miss company: {}\".format(len(missedCompany)))\n",
    "\n",
    "###todo 这些股票查不到，这些公司基本都是2023年上市的，可能数据还没更新吧"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The number of industry: {}\".format(len(theBindDataInTushare['industry'].unique())))\n",
    "print(\"The exact number of company: {}\".format(len(theBindDataInTushare['ts_code'].unique())))\n",
    "# print(\"--------------------------------------------------------\\n\"+\"The industries: {}\".format(theBindDataInTushare['industry'].unique()))\n",
    "\n",
    "theCounter={}\n",
    "uniqueList=rawCompanyDataInTushare['industry'].values\n",
    "for industry in uniqueList:\n",
    "    if industry in theCounter:\n",
    "        theCounter[industry]+=1\n",
    "    else:\n",
    "        theCounter[industry]=1\n",
    "theCounter=sorted(theCounter.items(), key = lambda kv:kv[1],reverse=True)\n",
    "print(\"--------------------------------------------------------\\n\"+\"the number of company in different industry:\\n {}\".format(theCounter))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### test science data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TCLStock=theBindDataInTushare[theBindDataInTushare['ts_code']==\"000100.SZ\"].sort_values(by='trade_date',ascending=True)\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.gca()\n",
    "locator = mdates.DayLocator(interval=30*3)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.plot(TCLStock['trade_date'],TCLStock['high'],label=\"high\")\n",
    "plt.plot(TCLStock['trade_date'],TCLStock['low'],label=\"low\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.title('TCL100')\n",
    "plt.legend()   #打上标签\n",
    "plt.show()\n",
    "TCLStock"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.1 大盘整体趋势"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theTrendResult=theBindDataInTushare.groupby('trade_date')[['close','open']].mean()\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "locator = mdates.DayLocator(interval=30*3)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.plot(theTrendResult['open'],label=\"open\")\n",
    "plt.plot(theTrendResult['close'],label=\"close\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Amount (RMB)\")\n",
    "plt.title('The Trend')\n",
    "plt.legend()   #打上标签\n",
    "plt.show()\n",
    "\n",
    "theTrendResult"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.2 各个行业领域大盘\n",
    "\n",
    "注意: 该cell运行会绘制大量的图表，图表都被保存好了，如果不想运行可以查看路径stockData/tableForIndustry/raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###生成各个领域的平均总览图标\n",
    "#\n",
    "# for i in list(theIndustryGroupedTrendResult):\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     x = np.arange(0, 300, 20)\n",
    "#     plt.yticks(x)\n",
    "#     i[-1]['close'].plot()\n",
    "#     plt.xlabel(\"Date\")\n",
    "#     plt.ylabel(\"Amount (RMB)\")\n",
    "#     plt.title(\"The Industry Trend: {}\".format(i[-1]['industry'].unique()[0]))\n",
    "#     plt.legend()   #打上标签\n",
    "#\n",
    "#     ###下载图表\n",
    "#     # plt.savefig(\"stockData/tableForIndustry/raw/industryTablesFor{}.jpg\".format(format(i[-1]['industry'].unique()[0])))\n",
    "#\n",
    "# plt.show()\n",
    "#\n",
    "# theIndustryTrendResult"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1.3 数据异常处理 (industry 为 none，建议优化)\n",
    "注意: 行业总共110种，很多，在模型里面本身是建议降维的; 而且有些行业重复，例如金属被分成好几个领域，但是不建议归类，因为这些行业的走势都不一样，建议保留，等后续讨论。\n",
    "\n",
    "目前计划拿到这些公司的产品与介绍，nle分析并将其分类到合适的种类\n",
    "\n",
    "目前有两个方向，一个根据tf-idf来量化词语，一个是通过word2vec来量化(此方案为back up，原因不够精确)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 目前已知的行业有110种，还有一种没有归类，现在尝试归类\n",
    "print(\"the number of company with none industry: {}\".format(len(rawCompanyDataInTushare[rawCompanyDataInTushare['industry'] == 'None']['ts_code'].values)))\n",
    "\n",
    "\n",
    "# ##查询行业缺失的公司简介与产品名称\n",
    "# noneIndustryCodeList=rawCompanyDataInTushare[rawCompanyDataInTushare['industry'] == 'None']['ts_code'].values\n",
    "# noneIndustryDF=pd.DataFrame()\n",
    "# now=datetime.now()\n",
    "# for code in noneIndustryCodeList:\n",
    "#     ###该接口每分钟最多10次，尼玛得搞16分钟，之后会存在stockData/CompanyIntroductionWithNoneIndustry.csv\n",
    "#     noneIndustrydf = pro.stock_company(ts_code=code, fields='ts_code,introduction,main_business')\n",
    "#     noneIndustryDF=pd.concat([noneIndustryDF,noneIndustrydf])\n",
    "#     time.sleep(6)\n",
    "#\n",
    "# noneIndustryDF.to_csv(\"stockData/CompanyIntroductionWithNoneIndustry.csv\")\n",
    "# print(\"cost time: {}\".format(datetime.now()-now))\n",
    "#\n",
    "#\n",
    "#\n",
    "# ##查询行业已知的公司简介与产品名称\n",
    "# knownIndustryCodeList=rawCompanyDataInTushare[rawCompanyDataInTushare['industry'] != 'None']['ts_code'].values\n",
    "# knownIndustryDF=pd.DataFrame()\n",
    "# now=datetime.now()\n",
    "# for code in knownIndustryCodeList:\n",
    "#     ###该接口每分钟最多10次，尼玛得搞16分钟，之后会存在stockData/CompanyIntroductionWithNoneIndustry.csv\n",
    "#     knownIndustrydf = pro.stock_company(ts_code=code, fields='ts_code,introduction,main_business')\n",
    "#     knownIndustryDF=pd.concat([knownIndustryDF,knownIndustrydf])\n",
    "#     time.sleep(6)\n",
    "#\n",
    "# knownIndustryDF.to_csv(\"stockData/CompanyIntroductionWithKnownIndustry.csv\")\n",
    "# print(\"cost time: {}\".format(datetime.now()-now))\n",
    "\n",
    "\n",
    "noneIndustryDF=pd.read_csv(\"stockData/CompanyIntroductionWithNoneIndustry.csv\", index_col=False)\n",
    "knownIndustryDF=pd.read_csv(\"stockData/CompanyIntroductionWithKnownIndustry.csv\", index_col=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "noneIndustryDF['industry']=noneIndustryDF.apply(lambda x:rawCompanyDataInTushare[x['ts_code']==rawCompanyDataInTushare['ts_code']]['industry'].values[0],axis=1)\n",
    "\n",
    "noneIndustryDF"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knownIndustryDF['industry']=knownIndustryDF.apply(lambda x:rawCompanyDataInTushare[x['ts_code']==rawCompanyDataInTushare['ts_code']]['industry'].values[0],axis=1)\n",
    "knownIndustryDF=knownIndustryDF[knownIndustryDF['industry']!='None']\n",
    "print(\"行业数量（不包括None）: {}\\n行业:{}\".format(len(knownIndustryDF['industry'].unique()),knownIndustryDF['industry'].unique()))\n",
    "knownIndustryDF"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 由于有110个行业，有的重复度比较高，我打算降维一下\n",
    "def decreseIndustrDiamantion(theData,name):\n",
    "    theDecreased={'钢材':['特种钢','钢加工','普钢'],'酒类':['白酒','啤酒','红黄酒'],'矿业':['铝锌','铜','铝','黄金','小金属'],'石油':['石油加工','石油贸易','石油开采'],'电力':['火力发电','新型电力'],'机械':['轻工机械','纺织机械','工程机械','专用机械','机械基件'],'运输':['水运','空运','仓储物流'],'地产':['全国地产','区域地产']}\n",
    "    for thisIndex, thisRow in theData.iterrows():\n",
    "        for key, value in theDecreased.items():\n",
    "            if thisRow[name] in value:\n",
    "                thisRow[name]=key\n",
    "                theData.loc[thisIndex]=thisRow\n",
    "    return theData"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "###分词\n",
    "import jieba\n",
    "import re\n",
    "import jieba.analyse as analyse\n",
    "\n",
    "for industry in knownIndustryDF['industry'].unique():\n",
    "    jieba.add_word(industry, freq=None, tag=None)\n",
    "\n",
    "\n",
    "def getStopWords(url):\n",
    "    with open(url,'r',encoding='utf8') as f:\n",
    "        stopword = f.readlines()\n",
    "    return [i.replace('\\n','') for i in stopword]\n",
    "\n",
    "def tokenizPara(text,stopword,fin_length=0.6):\n",
    "    text=text.replace(\".\",\" \")\n",
    "    URL_REGEX = re.compile(\n",
    "    r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "    re.IGNORECASE)\n",
    "    text = re.sub(URL_REGEX, \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    sentences=re.split('，|_|-|!|。|,|/|：|:',text.strip())\n",
    "    cutSentences=[]\n",
    "\n",
    "    ###先分词\n",
    "    for sentence in sentences:\n",
    "        ex=\"///\".join(jieba.cut(sentence,cut_all=False))\n",
    "        cutSentences+=ex.split('///')\n",
    "\n",
    "    ###先去除停用词与一些数字，拿到提纯的文本\n",
    "    afterStop=[i for i in cutSentences if len(i)>1 and not i.isdigit() and i not in stopword]\n",
    "\n",
    "    ###二次提纯，根据比例提出tf-idf分数高于平均的词语，并且词性作筛选,并且至少留住50%个词\n",
    "    afterTfidf=dict(analyse.extract_tags(\" \".join(afterStop), withWeight=True, allowPOS=('ns', 'n', 'vn')))\n",
    "    averTfidf=np.mean(list(afterTfidf.values()))\n",
    "    if len([k for k, v in afterTfidf.items() if v>=averTfidf]) >=len(afterStop)*fin_length:\n",
    "        # print([k for k, v in afterTfidf.items() if v>=averTfidf])\n",
    "        return [k for k, v in afterTfidf.items() if v>=averTfidf]\n",
    "    else:\n",
    "        afterTfidf=dict(sorted(afterTfidf.items(), key = lambda kv:kv[1],reverse=True))\n",
    "        # print(list(afterTfidf.keys())[:math.ceil(len(afterStop)*fin_length)])\n",
    "        return list(afterTfidf.keys())[:math.ceil(len(afterStop)*fin_length)]\n",
    "    # return [k for k, v in afterTfidf.items()]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 加载stopword\n",
    "cn_stopwords=getStopWords(\"NLP/stopwords/cn_stopwords.txt\")\n",
    "baidu_stopword=getStopWords(\"NLP/stopwords/baidu_stopwords.txt\")\n",
    "hit_stopword=getStopWords(\"NLP/stopwords/hit_stopwords.txt\")\n",
    "scu_stopword=getStopWords(\"NLP/stopwords/scu_stopwords.txt\")\n",
    "myStopword=['一家','','目前','公司','产品','企业','生产','山东','上市','坚实','每天','天天','完全','没有','众多','提供','完全','以古','地方','典当','拥有','下属','固有','无处不在','行业','销售','业务','主要','主要业务']\n",
    "limitedIndustryList=['机场','批发业','陶瓷','商品城','林业','电器连锁','公路']\n",
    "stopword_list=cn_stopwords+hit_stopword+baidu_stopword+scu_stopword+myStopword\n",
    "for stopword in stopword_list:\n",
    "    jieba.add_word(stopword, freq=None, tag=None)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 开始分词\n",
    "unknownIndustryParagraphs=[]\n",
    "unknownIndustryCutKeyword=[]\n",
    "for item in noneIndustryDF.values:\n",
    "    # item2=\"\"\n",
    "    # if item[2]!=np.nan and item[2]!=np.NAN and item[2]!=\"nan\":\n",
    "    #     item2=item[2]\n",
    "    tokens=tokenizPara(str(item[1])+str(item[2]),stopword_list)\n",
    "    unknownIndustryParagraphs.append(tokens)\n",
    "    unknownIndustryCutKeyword.append(\" \".join(tokens))\n",
    "\n",
    "knownIndustryParagraphs=[]\n",
    "knownIndustryCutKeyword=[]\n",
    "for item in knownIndustryDF.values:\n",
    "    # item2=\"\"\n",
    "    # if item[2]!=np.nan and item[2]!=np.NAN and item[2]!=\"nan\":\n",
    "    #     item2=item[2]\n",
    "    tokens=tokenizPara(str(item[1])+str(item[2]),stopword_list)\n",
    "    knownIndustryParagraphs.append(tokens)\n",
    "    knownIndustryCutKeyword.append(\" \".join(tokens))\n",
    "\n",
    "\n",
    "# print(\"------------------------------------\\n\"\n",
    "#       \"unknowIndustry:\\n{}\"\n",
    "#       \"\\n------------------------------------\\n\"\n",
    "#       \"knowIndustry:\\n{}\".format(unknownIndustryParagraphs[:3],knownIndustryParagraphs[:3]))\n",
    "\n",
    "knownIndustryDF['keyword']=knownIndustryParagraphs\n",
    "knownIndustryDF['cut_keywords']=knownIndustryCutKeyword\n",
    "noneIndustryDF['keyword']=unknownIndustryParagraphs\n",
    "noneIndustryDF['cut_keywords']=unknownIndustryCutKeyword"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo test jieba\n",
    "print(\"原文：\",knownIndustryDF['keyword'].values[0])\n",
    "testwordlist=\" \".join(knownIndustryDF['keyword'].values[0])\n",
    "keywords = analyse.extract_tags(testwordlist, topK=5, withWeight=False, allowPOS=('ns', 'n', 'vn'))\n",
    "print(\"tf-idf筛选后（最高的5个词语）：\",keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 行业降维\n",
    "knownIndustryDF=decreseIndustrDiamantion(knownIndustryDF,'industry')\n",
    "knownIndustryDF"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### todo 分类并合并数据\n",
    "# print(knownIndustryDF.head())\n",
    "tempdf=knownIndustryDF\n",
    "###根据行业合并文本，合并材料\n",
    "introductionByIndustry=pd.DataFrame()\n",
    "for intro in list(tempdf.groupby('industry')):\n",
    "    paras=[]\n",
    "    AllIntroduction=[]\n",
    "    for para in intro[1][['keyword','introduction']].values:\n",
    "        paras+=para[0]\n",
    "        AllIntroduction.append(para[1])\n",
    "\n",
    "    ###calculate word frequency\n",
    "    freq={}\n",
    "    for word in paras:\n",
    "        if word in freq:\n",
    "            freq[word]+=1\n",
    "        else:\n",
    "            freq[word]=1\n",
    "    freq=sorted(freq.items(), key = lambda kv:kv[1],reverse=True)\n",
    "    freq = dict(freq)\n",
    "    #('机场', 5), ('批发业', 5), ('陶瓷', 5), ('商品城', 4), ('林业', 4), ('电器连锁', 2), ('公路', 2)\n",
    "    ###开始排除部分词频为1的词语，减少影响，以上的行业关键词看情况减少，因为样本数量实在太少了\n",
    "    notIncludedIndustryWords=[]\n",
    "    # for key, value in freq.items():\n",
    "    #     if value==1:\n",
    "    #         notIncludedIndustryWords.append(key)\n",
    "    #         stopword_list.append(key)\n",
    "    # if intro[0] not in limitedIndustryList:\n",
    "    # print(\"industry: {}, notIncl: {}\".format(intro[0],notIncludedIndustryWords))\n",
    "    # paras=[x for x in paras if x not in notIncludedIndustryWords]\n",
    "    row=pd.DataFrame({'industry':intro[0],'keyword':[paras],'AllIntro':[AllIntroduction],'frequency':[freq]})\n",
    "    introductionByIndustry=pd.concat([row,introductionByIndustry])\n",
    "\n",
    "introductionByIndustry['cut_keywords']=introductionByIndustry.apply(lambda x:\" \".join(x['keyword']), axis=1)\n",
    "\n",
    "###\n",
    "#对未知行业的关键词也进行筛选\n",
    "noneIndustryDF['keyword']=noneIndustryDF['keyword'].apply(lambda x: [j for j in x if j not in stopword_list])\n",
    "knownIndustryDF['keyword']=knownIndustryDF['keyword'].apply(lambda x : [j for j in x if j not in stopword_list])\n",
    "introductionByIndustry"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "introductionByIndustry.loc[introductionByIndustry[\"industry\"].isin(limitedIndustryList)]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 公司行业归类(tf-idf) (废弃，该算法已融入进下一个方案)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###量化词语成tfidf分数\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer,CountVectorizer\n",
    "corpus_list=knownIndustryDF['cut_keywords'].values.tolist()+noneIndustryDF['cut_keywords'].values.tolist()\n",
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "tfidf = vectorizer.fit_transform(corpus_list)\n",
    "weight = tfidf.toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"\\n词汇表大小：{}\\n{}\".format(len(vocab),vocab))\n",
    "print(\"\\n权重形状：{}\\n{}\".format(weight.shape,weight))\n",
    "weight"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 公司行业归类(word2vec词向量)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, word2vec\n",
    "# ### todo 训练词向量库\n",
    "# sentences=knownIndustryDF['keyword'].values.tolist()+noneIndustryDF['keyword'].values.tolist()\n",
    "# model = word2vec.Word2Vec(sentences,min_count=1, vector_size=200,sg=1)\n",
    "#\n",
    "# ### todo 保存词向量库\n",
    "# model.wv.save('NLP/wordVector/MineWordVec/MyWordModel.model')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 准备词向量库\n",
    "###100向量\n",
    "# model = KeyedVectors.load_word2vec_format('NLP/wordVector/tencent-ailab-embedding-zh-d100-v0.2.0/tencent-ailab-embedding-zh-d100-v0.2.0.txt', binary=False)\n",
    "\n",
    "###200向量\n",
    "model = KeyedVectors.load_word2vec_format('NLP/wordVector/tencent-ailab-embedding-zh-d200-v0.2.0/tencent-ailab-embedding-zh-d200-v0.2.0.txt', binary=False)\n",
    "\n",
    "###自己的词库，不怎么样\n",
    "# model=KeyedVectors.load('NLP/wordVector/MineWordVec/MyWordModel.model')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 把词库里没有的词去掉\n",
    "noneIndustryDF['keyword']=noneIndustryDF['keyword'].apply(lambda x: [j for j in x if j in model])\n",
    "knownIndustryDF['keyword']=knownIndustryDF['keyword'].apply(lambda x : [j for j in x if j in model])\n",
    "introductionByIndustry['keyword']=introductionByIndustry['keyword'].apply(lambda x : [j for j in x if j in model])\n",
    "###todo 去掉不相干词语\n",
    "# knownIndustryDF['keyword']=knownIndustryDF['keyword'].apply(lambda x:[j for j in x if j!=model.doesnt_match(x)])\n",
    "# introductionByIndustry['keyword']=introductionByIndustry['keyword'].apply(lambda x:[j for j in x if j!=model.doesnt_match(x)])\n",
    "# noneIndustryDF['keyword']=noneIndustryDF['keyword'].apply(lambda x:[j for j in x if j!=model.doesnt_match(x)])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "largestWordInKnow=0\n",
    "for paragra in knownIndustryDF['keyword'].values:\n",
    "    if len(paragra)>largestWordInKnow:\n",
    "        largestWordInKnow=len(paragra)\n",
    "\n",
    "largestWordInUnknow=0\n",
    "for paragra in noneIndustryDF['keyword'].values:\n",
    "    if len(paragra)>largestWordInUnknow:\n",
    "        largestWordInUnknow=len(paragra)\n",
    "\n",
    "largestWordInIntro=0\n",
    "for paragra in introductionByIndustry['keyword'].values:\n",
    "    if len(paragra)>largestWordInIntro:\n",
    "        largestWordInIntro=len(paragra)\n",
    "\n",
    "print(\"The largest num of word in Known: {}\".format(largestWordInKnow))\n",
    "print(\"The largest num of word in unKnown: {}\".format(largestWordInUnknow))\n",
    "print(\"The largest num of word in Introduction: {}\".format(largestWordInIntro))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##[\"我不看好当前股票市场\"] negative -1.0 -4.0\n",
    "###[\"我对当前市场持积极股票\"] positive 2.0 5.0\n",
    "###[\"我保持观望态度\"] neutral -1 1\n",
    "\n",
    "\n",
    "###wordbag:['失望':-0.7,.....]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 方案一: 直接调用接口（目前最优解）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo直接测试句向量similarity,看看准确率\n",
    "###这个cell的对比方法是[所有相同行业的公司简介分词]:[当前某个None行业公司简介分词]\n",
    "def getWordVector(keywords=None,w2v_model=model):\n",
    "    results={}\n",
    "    for index, row in introductionByIndustry.iterrows():\n",
    "        results[row['industry']]=w2v_model.n_similarity(keywords,row['keyword'])\n",
    "    results=sorted(results.items(), key = lambda kv:kv[1],reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"第一种方式:\\n\")\n",
    "i=0\n",
    "for index, row in noneIndustryDF.iterrows():\n",
    "    if i==5:\n",
    "        break\n",
    "    else:\n",
    "        i+=1\n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"第{}原文: {}\".format(i,row['introduction']))\n",
    "    results=getWordVector(keywords=row['keyword'])\n",
    "    # print(\"原分类:\")\n",
    "    # print(row['industry'])\n",
    "    print(\"相似度:\")\n",
    "    print(results[:10])\n",
    "\n",
    "# correct_num=0\n",
    "# for index, row in knownIndustryDF.iterrows():\n",
    "#     results=dict(getWordVector(keywords=row['keyword']))\n",
    "#     if next(iter(results.keys()))==row['industry']:\n",
    "#         correct_num+=1\n",
    "# print(\"accuracy score: {}%\".format((correct_num/len(knownIndustryDF['industry'].values)*100)))\n",
    "###腾讯词库准确率:52.54%\n",
    "###自己训练的词库: 35.38%\n",
    "\n",
    "###200向量的准确率 56.69%"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###这个cell的对比方法是[单个行业的某个公司简介分词]:[当前某个None行业公司简介分词]\n",
    "###最后取平均值\n",
    "print(\"第二种方式:\\n\")\n",
    "i=0\n",
    "for index, row in noneIndustryDF.iterrows():\n",
    "    if i==5:\n",
    "        break\n",
    "    else:\n",
    "        i+=1\n",
    "\n",
    "    print(\"第{}原文: {}\".format(i,row['introduction']))\n",
    "\n",
    "    meanScore={}\n",
    "    for intro in list(knownIndustryDF.groupby('industry')):\n",
    "        scores=[]\n",
    "        for para in intro[1][['industry','keyword']].values:\n",
    "            scores.append(model.n_similarity(para[0],row['keyword']))\n",
    "        meanScore[intro[1]['industry'].values[0]]=sum(scores)/len(scores)\n",
    "    meanScore=sorted(meanScore.items(),key=lambda k:k[1],reverse=True)\n",
    "    print(\"相似度: \\n{}\".format(meanScore[:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 方案二: Xgboost\n",
    "上面两个cell的结果是直接用句向量数值进行比较，得出每个行业的相似度，相似度越高，与这个行业契合度越高，然后每个cell都有一种方法(在注释)。可是经过测试证明，第二个cell采取平均值完全没用，第一个cell虽然能有效地把公司简介主题归化从110个到5-2个左右，但是到了这个范围，准确率就不高了，只能说取得了初步效果，但是不是很理想\n",
    "\n",
    "第二次测试一下神经网络与xgboost，不过需要把每个词都改成向量，而不是句向量\n",
    "\n",
    "一开始直接把similarity输进去训练，准确率只有46，方法不对，不如直接调用word2vec接口\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 准备训练输入数据\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "def average_word_vectors(words,model,vocabulary,num_features):\n",
    "    feature_vector=np.zeros((num_features,),dtype='float64')\n",
    "    nwords=0\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords=nwords+1\n",
    "            feature_vector=np.add(feature_vector,model[word])\n",
    "    if nwords:\n",
    "        feature_vector=np.divide(feature_vector,nwords)\n",
    "    return feature_vector\n",
    "\n",
    "###a: 145word word:200 vector\n",
    "###b: 155wod word:200 vect\n",
    "###a:[] shape(155)   b:[] shape(155)\n",
    "###a sum[vector]/word_count\n",
    "### 200\n",
    "###\n",
    "\n",
    "# def average_word_vectors(words,model,vocabulary,num_features):\n",
    "#     feature_vector=np.zeros((num_features,),dtype='float64')\n",
    "#     n_words=0\n",
    "#     for word in words:\n",
    "#         feature_vector[n_words]=round(model[word].mean(),5)\n",
    "#         n_words+=1\n",
    "#         if n_words==num_features:\n",
    "#             break\n",
    "#     return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus,model,num_features):\n",
    "    #get the all vocabulary\n",
    "    vocabulary=model\n",
    "    features=[average_word_vectors(tokenized_sentence,model,vocabulary,num_features) for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# def get_word_vectors(data,model):\n",
    "#     words_art=[]\n",
    "#     for i in range(len(data)):\n",
    "#         words_art.append(eval(data.loc[i]))\n",
    "#     return averaged_word_vectorizer(words_art,model,num_features=300)\n",
    "\n",
    "###todo 词向量模型分析,准备数据输入\n",
    "### 量化keyword\n",
    "knownIndustryDF['keyword_vector']=knownIndustryDF['keyword'].apply(lambda x:[model.get_vector(j) for j in x])\n",
    "# knownIndustryDF['keyword_vector_mean']=knownIndustryDF['keyword'].apply(lambda x : [model.get_vector(i).mean() for i in x])\n",
    "# knownIndustryDF['keyword_length']=knownIndustryDF['keyword'].apply(lambda x: len(x))\n",
    "# knownIndustryDF['sentence_vector_sum']=knownIndustryDF['keyword_vector'].apply(lambda x : sum(x))\n",
    "# knownIndustryDF['sentence_vector_mean']=knownIndustryDF['keyword_vector'].apply(lambda x : sum(x)/len(x))\n",
    "# ###把句向量也引进来\n",
    "# for index, row in introductionByIndustry.iterrows():\n",
    "#     knownIndustryDF['n_similarity_'+row['industry']]=knownIndustryDF['keyword'].apply(lambda x: model.n_similarity(x,row['keyword']))\n",
    "\n",
    "# TheDataSet=pd.read_csv(\"stockData/keywordVecIntroductionWithKnownIndustry.csv\")\n",
    "# TheDataSet=TheDataSet.drop(labels=['keyword_vector','keyword_vector_mean','industry','main_business','introduction','ts_code','keyword'], axis=1)\n",
    "\n",
    "TheDataSet=knownIndustryDF\n",
    "x=averaged_word_vectorizer(knownIndustryDF['keyword'].values,model,model.vector_size)\n",
    "\n",
    "\n",
    "###one-hot\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y=encoder.fit_transform([[x] for x in TheDataSet['industry'].values])\n",
    "# x=TheDataSet\n",
    "###分割数据集\n",
    "x_train , x_test , y_train , y_test = train_test_split(x,y, test_size= 0.1 ,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###准备参数,开始训练\n",
    "# param_grid = {\n",
    "#             'max_depth': [2, 3, 4, 5, 6, 7, 8],\n",
    "#             'n_estimators': [30, 50, 100, 300, 500, 1000,2000],\n",
    "#             'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.03, 0.05, 0.5],\n",
    "#             \"gamma\":[0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "#             \"reg_alpha\":[0.0001,0.001, 0.01, 0.1, 1, 100],\n",
    "#             \"reg_lambda\":[0.0001,0.001, 0.01, 0.1, 1, 100],\n",
    "#             \"min_child_weight\": [2,3,4,5,6,7,8],\n",
    "#             \"colsample_bytree\": [0.6, 0.7, 0.8, 0.9],\n",
    "#             \"subsample\":[0.6, 0.7, 0.8, 0.9]\n",
    "# }\n",
    "param_grid = {\n",
    "            'max_depth': [2, 4, 6, 8],\n",
    "            'n_estimators': [50, 100, 300, 500,1000],\n",
    "            \"gamma\":[0.1, 0.2, 0.3],\n",
    "            \"min_child_weight\": [2,4,6,8],\n",
    "}\n",
    "\n",
    "# other_params = {'learning_rate': 0.3, 'n_estimators': 400, 'max_depth': 5, 'min_child_weight': 1, 'seed': 42, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "# grid_xgboost = GridSearchCV(estimator=xgb.XGBRegressor(seed=27), param_grid=param_grid, cv=5)\n",
    "# multioutputregressor = MultiOutputRegressor(grid_xgboost).fit(x_train, y_train)\n",
    "\n",
    "multioutputregressor = MultiOutputRegressor(xgb.XGBRegressor(max_depth=5,min_child_weight=3,learning_rate=0.1,n_estimators=400,objective='binary:logistic')).fit(x_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 熟料 发贴    [熟料 发贴]\n",
    "###熟料          [1,0]\n",
    "###发贴          [0,1]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###todo 验证\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "# def get_scores(preds,gt):\n",
    "#     # print ('AUC: %.4f' % metrics.roc_auc_score(gt,preds))\n",
    "#     print ('ACC: %.4f' % metrics.accuracy_score(gt,preds))\n",
    "#     print('macro')\n",
    "#     print( 'Recall: %.4f' % metrics.recall_score(y_test,preds,average='macro'))\n",
    "#     print( 'F1-score: %.4f' %metrics.f1_score(gt,preds,average='macro'))\n",
    "#     print( 'Precision: %.4f' %metrics.precision_score(gt,preds,average='macro'))\n",
    "#\n",
    "#     print('\\nmicro:')\n",
    "#     print( 'Recall: %.4f' % metrics.recall_score(y_test,preds,average='micro'))\n",
    "#     print( 'F1-score: %.4f' %metrics.f1_score(gt,preds,average='micro'))\n",
    "#     print( 'Precision: %.4f' %metrics.precision_score(gt,preds,average='micro'))\n",
    "\n",
    "check=multioutputregressor.predict(x_test)\n",
    "for i in range(len(check)):\n",
    "        max_value=max(check[i])\n",
    "        for j in range(len(check[i])):\n",
    "            if max_value==check[i][j]:\n",
    "                check[i][j]=1\n",
    "            else:\n",
    "                check[i][j]=0\n",
    "\n",
    "print(classification_report(y_test, check))\n",
    "mse = mean_squared_error(y_test, check)\n",
    "print(\"mse score: {}%\".format(round(mse*100,2)))\n",
    "\n",
    "result={\"result\":[j[0] for j in encoder.inverse_transform(check)],\"test_answer\":[j[0] for j in encoder.inverse_transform(y_test)]}\n",
    "\n",
    "result_check=pd.DataFrame(result)\n",
    "\n",
    "accurate_num=0\n",
    "for index, element in enumerate(encoder.inverse_transform(y_test)):\n",
    "    if element==encoder.inverse_transform(check)[index]:\n",
    "        accurate_num+=1\n",
    "\n",
    "print(\"accuracy score: {}%\".format(round((accurate_num/len(y_test))*100,2)))\n",
    "result_check\n",
    "### word2vec & xgboost 表现不是很好, mse:0.65%, 准确率 47%，不过个人认为方法完全错了"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 方案三: svm 向量机"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = MultiOutputRegressor(svm.SVC(C=5, gamma= 1, probability=True)).fit(x_train, y_train)\n",
    "y_predict = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_predict))\n",
    "\n",
    "for i in range(len(y_predict)):\n",
    "        max_value=max(y_predict[i])\n",
    "        for j in range(len(y_predict[i])):\n",
    "            if max_value==y_predict[i][j]:\n",
    "                y_predict[i][j]=1\n",
    "            else:\n",
    "                y_predict[i][j]=0\n",
    "result={\"result\":[j[0] for j in encoder.inverse_transform(y_predict)],\"test_answer\":[j[0] for j in encoder.inverse_transform(y_test)]}\n",
    "\n",
    "result_check=pd.DataFrame(result)\n",
    "\n",
    "accurate_num=0\n",
    "for index, element in enumerate(encoder.inverse_transform(y_test)):\n",
    "    if element==encoder.inverse_transform(y_predict)[index]:\n",
    "        accurate_num+=1\n",
    "\n",
    "print(\"accuracy score: {}%\".format(round((accurate_num/len(y_test))*100,2)))\n",
    "result_check"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 方案四: textCnn\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense,Dropout,Activation,Input, Lambda, Reshape,concatenate\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D,GlobalMaxPooling1D,Flatten,BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "\n",
    "\n",
    "#导入word2vec模型并进行预处理\n",
    "def w2v_model_preprocessing(content,w2v_model,max_len=32):\n",
    "    # 初始化 `[word : index]` 字典\n",
    "    word2idx = {\"_PAD\": 0}\n",
    "    # 训练数据 词汇表构建\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(content)\n",
    "    vocab_size = len(tokenizer.word_index)  # 词库大小\n",
    "    print(tokenizer.word_index)\n",
    "    error_count = 0\n",
    "    # 存储所有 word2vec 中所有向量的数组，其中多一位，词向量全为 0， 用于 padding\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, w2v_model.vector_size))\n",
    "    print(embedding_matrix.shape)\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in w2v_model:\n",
    "            embedding_matrix[i] = w2v_model[word]\n",
    "        else:\n",
    "            error_count += 1\n",
    "    # 训练数据 词向量截断补全(padding)\n",
    "    seq = tokenizer.texts_to_sequences(content)\n",
    "    trainseq = pad_sequences(seq, maxlen=max_len,padding='post')\n",
    "    return embedding_matrix,trainseq\n",
    "\n",
    "\n",
    "def build_textcnn(max_len,embeddings_dim,embeddings_matrix):\n",
    "    #构建textCNN模型\n",
    "    main_input = Input(shape=(max_len,), dtype='float64')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(\n",
    "                         len(embeddings_matrix), #表示文本数据中词汇的取值可能数,从语料库之中保留多少个单词\n",
    "                         embeddings_dim, # 嵌入单词的向量空间的大小\n",
    "                         input_length=max_len, #规定长度\n",
    "                         weights=[embeddings_matrix],# 输入序列的长度，也就是一次输入带有的词汇个数\n",
    "                         trainable=False # 设置词向量不作为参数进行更新\n",
    "                         )\n",
    "    embed = embedder(main_input)\n",
    "    flat = Flatten()(embed)\n",
    "    dense01 = Dense(5096, activation='relu')(flat)\n",
    "    dense02 = Dense(1024, activation='relu')(dense01)\n",
    "    main_output = Dense(2, activation='softmax')(dense02)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "embedding_matrix,trainSet=w2v_model_preprocessing(knownIndustryDF['keyword'].values,model)\n",
    "embedding_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果写到这一步，证明主题分类的模型已经写好了，虽然不知道质量怎么样hhh。其实说真的，缺失行业标记的公司不过才164家，人力做的话几天就能写完了，不过我没做过中文的词义分析，这算是一个nlp的开始吧，但是说真的太麻烦了，希望以后有高效的方法，"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### todo 泛化\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 use yfinance (雅虎非官方API) api get stock data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#测试api\n",
    "data = yf.download(tickers='AMD',start='2013-01-1',end='2023-01-01')\n",
    "print(\"The types in the data frame:\\n{}\".format(data.dtypes))\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "amd = yf.Ticker(\"AMD\")\n",
    "info=pd.DataFrame.from_dict(amd.info)\n",
    "print(\"The types in the data frame:\\n{}\".format(info.dtypes))\n",
    "info"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"Close\"].plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##获取标普500股票公司名字\n",
    "import bs4 as bs\n",
    "import requests\n",
    "\n",
    "###爬取维基百科标普500公司名称\n",
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "\n",
    "###筛选元素\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "   ticker = row.findAll('td')[0].text\n",
    "   ticker=ticker.replace(\"\\n\",\"\")\n",
    "   tickers.append(ticker)\n",
    "\n",
    "\n",
    "tickers"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def getStockDataInYFinance(name,start='2013-01-1',end='2023-01-01'):\n",
    "    includedList=['country','zip','industry','sector']\n",
    "    data = yf.download(tickers=name,start=start,end=end)\n",
    "    data['trade_date'] = data.index\n",
    "    company = yf.Ticker(name)\n",
    "    try:\n",
    "        companyInfo=pd.DataFrame.from_dict(company.info)\n",
    "    except BaseException:\n",
    "        print(company.info)\n",
    "        return np.nan\n",
    "    else:\n",
    "        data['name']=[name]*data.shape[0]\n",
    "        for column in companyInfo.columns:\n",
    "            if column in includedList:\n",
    "                data[column]=np.array([companyInfo[column].values[0]]).repeat(data.shape[0])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "### get all stock data and store them\n",
    "def storeAllStockDataForYFinance():\n",
    "    now=datetime.now()\n",
    "    allStockDataInYFinance=pd.DataFrame()\n",
    "    for company in tickers:\n",
    "        companyInfo=getStockDataInYFinance(company)\n",
    "        if companyInfo is np.nan:\n",
    "            print(\"break, company {} data is null\")\n",
    "            continue\n",
    "        else:\n",
    "            allStockDataInYFinance=pd.concat([allStockDataInYFinance,companyInfo])\n",
    "    print(\"Cost time: {}\".format(datetime.now()-now))\n",
    "    return allStockDataInYFinance"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "getStockDataInYFinance(tickers[0]).dtypes"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###仅用于下载yfinance 股票数据\n",
    "# stockDataInYFinance=storeAllStockDataForYFinance()\n",
    "# stockDataInYFinance.to_csv('stockData/YFinanceRawData.csv', header=stockDataInYFinance.columns,index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###验证一下有多少个查漏了\n",
    "stockDataInYFinance=pd.read_csv('stockData/YFinanceRawData.csv')\n",
    "uniqueCompany=stockDataInYFinance['name'].unique()\n",
    "missedList=[]\n",
    "for ticker in tickers:\n",
    "    if ticker not in uniqueCompany:\n",
    "        missedList.append(ticker)\n",
    "\n",
    "print(\"length of miss company: {}\".format(len(missedList)))\n",
    "###总共有3个公司遗漏\n",
    "\n",
    "###手动查询\n",
    "for company in missedList:\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"This is the information for {}\".format(company))\n",
    "    print(getStockDataInYFinance(name=company))\n",
    "    print(\"------------------------------------------------------\")\n",
    "###经过调查，属于信息不完善，有的连公司信息都不全"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"types in data: {}\".format(stockDataInYFinance.dtypes))\n",
    "stockDataInYFinance"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MMMStock=stockDataInYFinance[stockDataInYFinance['name']==\"MMM\"]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "ax = plt.gca()\n",
    "locator = mdates.DayLocator(interval=30*3)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "x=MMMStock['trade_date']\n",
    "\n",
    "plt.plot(x,MMMStock['High'],label=\"high\")\n",
    "plt.plot(x,MMMStock['Low'],label=\"low\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.title('MMM')\n",
    "plt.rc('xtick', labelsize=15)\n",
    "plt.rc('ytick', labelsize=15)\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "plt.legend()   #打上标签\n",
    "plt.show()\n",
    "MMMStock"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.0 Analysis the raw data in AI\n",
    "* data engineering\n",
    "* data cleaning\n",
    "* build model\n",
    "* train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------\n",
    "这里说一下训练集与验证集的分配:\n",
    "* 训练集: 截取 2013-01-01 to 2018-01-01，5年数据，占总比50%。\n",
    "* 验证集1: 截取 2018-01-01 to 2019-01-01 1年数据， 占总比10%，因为疫情就发生在2019年，该验证集用于检测模型准确度，确保该模型在疫情前时期可以正常使用\n",
    "* 验证集2: 截取 2019-01-01 to 2023-01-01 4年数据，占比40%，该数据集并非用于验证，而是在确保验证集1能正常运行的情况下，用于观察疫情发生与正常预期是否有大偏差（验证是否疫情对股票市场有巨大影响）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
