{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 本文的研究目的是 <code>情感分析</code>，但是通过大语言模型 <code>bert</code> 来做\n",
    "# 1.0 预训练与微调"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "###准备训练集与测试集\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:48:43.984589Z",
     "start_time": "2023-07-16T07:48:43.980683Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 15:48:46.873146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "sys.path.append('../bertTools')\n",
    "from bertTools.utils import logger_init\n",
    "from bertTools.model import BertConfig\n",
    "from bertTools.model import BertForPretrainingModel\n",
    "from bertTools.utils import LoadBertPretrainingDataset\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        # self.project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "        self.project_dir=os.getcwd()\n",
    "\n",
    "        # ========== wike2 数据集相关配置\n",
    "        # self.dataset_dir = os.path.join(self.project_dir, 'data', 'WikiText')\n",
    "        # self.pretrained_model_dir = os.path.join(self.project_dir, \"bert_base_uncased_english\")\n",
    "        # self.train_file_path = os.path.join(self.dataset_dir, 'wiki.train.tokens')\n",
    "        # self.val_file_path = os.path.join(self.dataset_dir, 'wiki.valid.tokens')\n",
    "        # self.test_file_path = os.path.join(self.dataset_dir, 'wiki.test.tokens')\n",
    "        # self.data_name = 'wiki2'\n",
    "\n",
    "        # ========== songci 数据集相关配置\n",
    "        self.dataset_dir = os.path.join(self.project_dir, 'bertTools', 'SongCi')\n",
    "        self.pretrained_model_dir = os.path.join(self.project_dir, 'bertTools',\"bert_google_1_L-12_H-768_A-12_cn\")\n",
    "        self.train_file_path = os.path.join(self.dataset_dir, 'songci.train.txt')\n",
    "        self.val_file_path = os.path.join(self.dataset_dir, 'songci.valid.txt')\n",
    "        self.test_file_path = os.path.join(self.dataset_dir, 'songci.test.txt')\n",
    "        self.data_name = 'model'\n",
    "\n",
    "        # 如果需要切换数据集，只需要更改上面的配置即可\n",
    "        self.vocab_path = os.path.join(self.pretrained_model_dir, 'vocab.txt')\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_save_dir = os.path.join(self.project_dir,'bertTools','result', 'cache')\n",
    "        self.logs_save_dir = os.path.join(self.project_dir, 'bertTools','result','logs')\n",
    "        self.model_save_path = os.path.join(self.model_save_dir, 'modelResult',f'model_{self.data_name}.bin')\n",
    "        self.writer = SummaryWriter(f\"bertTools/result/{self.data_name}\")\n",
    "        self.is_sample_shuffle = True\n",
    "        self.use_embedding_weight = True\n",
    "        self.batch_size = 16\n",
    "        self.max_sen_len = None  # 为None时则采用每个batch中最长的样本对该batch中的样本进行padding\n",
    "        self.pad_index = 0\n",
    "        self.random_state = 2022\n",
    "        self.learning_rate = 4e-5\n",
    "        self.weight_decay = 0.1\n",
    "        self.masked_rate = 0.15\n",
    "        self.masked_token_rate = 0.8\n",
    "        self.masked_token_unchanged_rate = 0.5\n",
    "        self.log_level = logging.DEBUG\n",
    "        self.use_torch_multi_head = False  # False表示使用model/BasicBert/MyTransformer中的多头实现\n",
    "        self.epochs = 200\n",
    "        self.model_val_per_epoch = 1\n",
    "\n",
    "        logger_init(log_file_name=self.data_name, log_level=self.log_level,\n",
    "                    log_dir=self.logs_save_dir)\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        bert_config_path = os.path.join(self.pretrained_model_dir, \"config.json\")\n",
    "        bert_config = BertConfig.from_json_file(bert_config_path)\n",
    "        for key, value in bert_config.__dict__.items():\n",
    "            self.__dict__[key] = value\n",
    "        # 将当前配置打印到日志文件中\n",
    "        logging.info(\" ### 将当前配置打印到日志文件中 \")\n",
    "        for key, value in self.__dict__.items():\n",
    "            logging.info(f\"### {key} = {value}\")\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    model = BertForPretrainingModel(config,\n",
    "                                    config.pretrained_model_dir)\n",
    "    last_epoch = -1\n",
    "    if os.path.exists(config.model_save_path):\n",
    "        checkpoint = torch.load(config.model_save_path)\n",
    "        last_epoch = checkpoint['last_epoch']\n",
    "        loaded_paras = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## 成功载入已有模型，进行追加训练......\")\n",
    "    model = model.to(config.device)\n",
    "    model.train()\n",
    "    bert_tokenize = BertTokenizer.from_pretrained(config.pretrained_model_dir).tokenize\n",
    "    data_loader = LoadBertPretrainingDataset(vocab_path=config.vocab_path,\n",
    "                                             tokenizer=bert_tokenize,\n",
    "                                             batch_size=config.batch_size,\n",
    "                                             max_sen_len=config.max_sen_len,\n",
    "                                             max_position_embeddings=config.max_position_embeddings,\n",
    "                                             pad_index=config.pad_index,\n",
    "                                             is_sample_shuffle=config.is_sample_shuffle,\n",
    "                                             random_state=config.random_state,\n",
    "                                             data_name=config.data_name,\n",
    "                                             masked_rate=config.masked_rate,\n",
    "                                             masked_token_rate=config.masked_token_rate,\n",
    "                                             masked_token_unchanged_rate=config.masked_token_unchanged_rate)\n",
    "    train_iter, test_iter, val_iter = \\\n",
    "        data_loader.load_train_val_test_data(test_file_path=config.test_file_path,\n",
    "                                             train_file_path=config.train_file_path,\n",
    "                                             val_file_path=config.val_file_path)\n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "            \"initial_lr\": config.learning_rate\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"initial_lr\": config.learning_rate\n",
    "        },\n",
    "    ]\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer,\n",
    "                                                          int(len(train_iter) * 0),\n",
    "                                                          int(config.epochs * len(train_iter)),\n",
    "                                                          last_epoch=last_epoch)\n",
    "    max_acc = 0\n",
    "    state_dict = None\n",
    "    for epoch in range(config.epochs):\n",
    "        losses = 0\n",
    "        start_time = time.time()\n",
    "        for idx, (b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(train_iter):\n",
    "            b_token_ids = b_token_ids.to(config.device)  # [src_len, batch_size]\n",
    "            b_segs = b_segs.to(config.device)\n",
    "            b_mask = b_mask.to(config.device)\n",
    "            b_mlm_label = b_mlm_label.to(config.device)\n",
    "            b_nsp_label = b_nsp_label.to(config.device)\n",
    "            loss, mlm_logits, nsp_logits = model(input_ids=b_token_ids,\n",
    "                                                 attention_mask=b_mask,\n",
    "                                                 token_type_ids=b_segs,\n",
    "                                                 masked_lm_labels=b_mlm_label,\n",
    "                                                 next_sentence_labels=b_nsp_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            losses += loss.item()\n",
    "            mlm_acc, _, _, nsp_acc, _, _ = accuracy(mlm_logits, nsp_logits, b_mlm_label,\n",
    "                                                    b_nsp_label, data_loader.PAD_IDX)\n",
    "            if idx % 20 == 0:\n",
    "                logging.info(f\"Epoch: [{epoch + 1}/{config.epochs}], Batch[{idx}/{len(train_iter)}], \"\n",
    "                             f\"Train loss :{loss.item():.3f}, Train mlm acc: {mlm_acc:.3f},\"\n",
    "                             f\"nsp acc: {nsp_acc:.3f}\")\n",
    "                config.writer.add_scalar('Training/Loss', loss.item(), scheduler.last_epoch)\n",
    "                config.writer.add_scalar('Training/Learning Rate', scheduler.get_last_lr()[0], scheduler.last_epoch)\n",
    "                config.writer.add_scalars(main_tag='Training/Accuracy',\n",
    "                                          tag_scalar_dict={'NSP': nsp_acc,\n",
    "                                                           'MLM': mlm_acc},\n",
    "                                          global_step=scheduler.last_epoch)\n",
    "        end_time = time.time()\n",
    "        train_loss = losses / len(train_iter)\n",
    "        logging.info(f\"Epoch: [{epoch + 1}/{config.epochs}], Train loss: \"\n",
    "                     f\"{train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\")\n",
    "        if (epoch + 1) % config.model_val_per_epoch == 0:\n",
    "            mlm_acc, nsp_acc = evaluate(config, val_iter, model, data_loader.PAD_IDX)\n",
    "            logging.info(f\" ### MLM Accuracy on val: {round(mlm_acc, 4)}, \"\n",
    "                         f\"NSP Accuracy on val: {round(nsp_acc, 4)}\")\n",
    "            config.writer.add_scalars(main_tag='Testing/Accuracy',\n",
    "                                      tag_scalar_dict={'NSP': nsp_acc,\n",
    "                                                       'MLM': mlm_acc},\n",
    "                                      global_step=scheduler.last_epoch)\n",
    "            # mlm_acc, nsp_acc = evaluate(config, train_iter, model, data_loader.PAD_IDX)\n",
    "            if mlm_acc > max_acc:\n",
    "                max_acc = mlm_acc\n",
    "                state_dict = deepcopy(model.state_dict())\n",
    "            torch.save({'last_epoch': scheduler.last_epoch,\n",
    "                        'model_state_dict': state_dict},\n",
    "                       config.model_save_path)\n",
    "\n",
    "\n",
    "def accuracy(mlm_logits, nsp_logits, mlm_labels, nsp_label, PAD_IDX):\n",
    "    \"\"\"\n",
    "    :param mlm_logits:  [src_len,batch_size,src_vocab_size]\n",
    "    :param mlm_labels:  [src_len,batch_size]\n",
    "    :param nsp_logits:  [batch_size,2]\n",
    "    :param nsp_label:  [batch_size]\n",
    "    :param PAD_IDX:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mlm_pred = mlm_logits.transpose(0, 1).argmax(axis=2).reshape(-1)\n",
    "    # 将 [src_len,batch_size,src_vocab_size] 转成 [batch_size, src_len,src_vocab_size]\n",
    "    mlm_true = mlm_labels.transpose(0, 1).reshape(-1)\n",
    "    # 将 [src_len,batch_size] 转成 [batch_size， src_len]\n",
    "    mlm_acc = mlm_pred.eq(mlm_true)  # 计算预测值与正确值比较的情况，得到预测正确的个数（此时还包括有mask位置）\n",
    "    mask = torch.logical_not(mlm_true.eq(PAD_IDX))  # 找到真实标签中，mask位置的信息。 mask位置为FALSE，非mask位置为TRUE\n",
    "    mlm_acc = mlm_acc.logical_and(mask)  # 去掉mlm_acc中mask的部分\n",
    "    mlm_correct = mlm_acc.sum().item()\n",
    "    mlm_total = mask.sum().item()\n",
    "    mlm_acc = float(mlm_correct) / mlm_total\n",
    "\n",
    "    nsp_correct = (nsp_logits.argmax(1) == nsp_label).float().sum()\n",
    "    nsp_total = len(nsp_label)\n",
    "    nsp_acc = float(nsp_correct) / nsp_total\n",
    "    return [mlm_acc, mlm_correct, mlm_total, nsp_acc, nsp_correct, nsp_total]\n",
    "\n",
    "\n",
    "def evaluate(config, data_iter, model, PAD_IDX):\n",
    "    model.eval()\n",
    "    mlm_corrects, mlm_totals, nsp_corrects, nsp_totals = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(data_iter):\n",
    "            b_token_ids = b_token_ids.to(config.device)  # [src_len, batch_size]\n",
    "            b_segs = b_segs.to(config.device)\n",
    "            b_mask = b_mask.to(config.device)\n",
    "            b_mlm_label = b_mlm_label.to(config.device)\n",
    "            b_nsp_label = b_nsp_label.to(config.device)\n",
    "            mlm_logits, nsp_logits = model(input_ids=b_token_ids,\n",
    "                                           attention_mask=b_mask,\n",
    "                                           token_type_ids=b_segs)\n",
    "            result = accuracy(mlm_logits, nsp_logits, b_mlm_label, b_nsp_label, PAD_IDX)\n",
    "            _, mlm_cor, mlm_tot, _, nsp_cor, nsp_tot = result\n",
    "            mlm_corrects += mlm_cor\n",
    "            mlm_totals += mlm_tot\n",
    "            nsp_corrects += nsp_cor\n",
    "            nsp_totals += nsp_tot\n",
    "    model.train()\n",
    "    return [float(mlm_corrects) / mlm_totals, float(nsp_corrects) / nsp_totals]\n",
    "\n",
    "\n",
    "def inference(config, sentences=None, masked=False, language='en', random_state=None):\n",
    "    \"\"\"\n",
    "    :param config:\n",
    "    :param sentences:\n",
    "    :param masked: 推理时的句子是否Mask\n",
    "    :param language: 语种\n",
    "    :param random_state:  控制mask字符时的随机状态\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bert_tokenize = BertTokenizer.from_pretrained(config.pretrained_model_dir).tokenize\n",
    "    data_loader = LoadBertPretrainingDataset(vocab_path=config.vocab_path,\n",
    "                                             tokenizer=bert_tokenize,\n",
    "                                             pad_index=config.pad_index,\n",
    "                                             random_state=config.random_state,\n",
    "                                             masked_rate=0.15)  # 15% Mask掉\n",
    "    token_ids, pred_idx, mask = data_loader.make_inference_samples(sentences,\n",
    "                                                                   masked=masked,\n",
    "                                                                   language=language,\n",
    "                                                                   random_state=random_state)\n",
    "    model = BertForPretrainingModel(config,\n",
    "                                    config.pretrained_model_dir)\n",
    "    if os.path.exists(config.model_save_path):\n",
    "        checkpoint = torch.load(config.model_save_path)\n",
    "        loaded_paras = checkpoint['model_state_dict']\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## 成功载入已有模型进行推理......\")\n",
    "    else:\n",
    "        raise ValueError(f\"模型 {config.model_save_path} 不存在！\")\n",
    "    model = model.to(config.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        token_ids = token_ids.to(config.device)  # [src_len, batch_size]\n",
    "        mask = mask.to(config.device)\n",
    "        mlm_logits, _ = model(input_ids=token_ids,\n",
    "                              attention_mask=mask)\n",
    "    pretty_print(token_ids, mlm_logits, pred_idx,\n",
    "                 data_loader.vocab.itos, sentences, language)\n",
    "\n",
    "\n",
    "def pretty_print(token_ids, logits, pred_idx, itos, sentences, language):\n",
    "    \"\"\"\n",
    "    格式化输出结果\n",
    "    :param token_ids:   [src_len, batch_size]\n",
    "    :param logits:  [src_len, batch_size, vocab_size]\n",
    "    :param pred_idx:   二维列表，每个内层列表记录了原始句子中被mask的位置\n",
    "    :param itos:\n",
    "    :param sentences: 原始句子\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    token_ids = token_ids.transpose(0, 1)  # [batch_size,src_len]\n",
    "    logits = logits.transpose(0, 1)  # [batch_size, src_len,vocab_size]\n",
    "    y_pred = logits.argmax(axis=2)  # [batch_size, src_len]\n",
    "    sep = \" \" if language == 'en' else \"\"\n",
    "    for token_id, sentence, y, y_idx in zip(token_ids, sentences, y_pred, pred_idx):\n",
    "        sen = [itos[id] for id in token_id]\n",
    "        sen_mask = sep.join(sen).replace(\" ##\", \"\").replace(\"[PAD]\", \"\").replace(\" ,\", \",\")\n",
    "        sen_mask = sen_mask.replace(\" .\", \".\").replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").lstrip()\n",
    "        logging.info(f\" ### 原始: {sentence}\")\n",
    "        logging.info(f\"  ## 掩盖: {sen_mask}\")\n",
    "        for idx in y_idx:\n",
    "            sen[idx] = itos[y[idx]].replace(\"##\", \"\")\n",
    "        sen = sep.join(sen).replace(\"[PAD]\", \"\").replace(\" ,\", \",\")\n",
    "        sen = sen.replace(\" .\", \".\").replace(\"[SEP]\", \"\").replace(\"[CLS]\", \"\").lstrip()\n",
    "        logging.info(f\"  ## 预测: {sen}\")\n",
    "        logging.info(\"===============\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:48:49.271061Z",
     "start_time": "2023-07-16T07:48:43.986844Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-16 15:48:49] - INFO: 成功导入BERT配置文件 /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/bert_google_1_L-12_H-768_A-12_cn/config.json\n",
      "[2023-07-16 15:48:49] - INFO:  ### 将当前配置打印到日志文件中 \n",
      "[2023-07-16 15:48:49] - INFO: ### project_dir = /Users/czc/PycharmProjects/stockMarketAnalysis\n",
      "[2023-07-16 15:48:49] - INFO: ### dataset_dir = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi\n",
      "[2023-07-16 15:48:49] - INFO: ### pretrained_model_dir = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/bert_google_1_L-12_H-768_A-12_cn\n",
      "[2023-07-16 15:48:49] - INFO: ### train_file_path = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi/songci.train.txt\n",
      "[2023-07-16 15:48:49] - INFO: ### val_file_path = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi/songci.valid.txt\n",
      "[2023-07-16 15:48:49] - INFO: ### test_file_path = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi/songci.test.txt\n",
      "[2023-07-16 15:48:49] - INFO: ### data_name = model\n",
      "[2023-07-16 15:48:49] - INFO: ### vocab_path = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/bert_google_1_L-12_H-768_A-12_cn/vocab.txt\n",
      "[2023-07-16 15:48:49] - INFO: ### device = cpu\n",
      "[2023-07-16 15:48:49] - INFO: ### model_save_dir = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/result/cache\n",
      "[2023-07-16 15:48:49] - INFO: ### logs_save_dir = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/result/logs\n",
      "[2023-07-16 15:48:49] - INFO: ### model_save_path = /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/result/cache/modelResult/model_model.bin\n",
      "[2023-07-16 15:48:49] - INFO: ### writer = <torch.utils.tensorboard.writer.SummaryWriter object at 0x7fb6a00ba700>\n",
      "[2023-07-16 15:48:49] - INFO: ### is_sample_shuffle = True\n",
      "[2023-07-16 15:48:49] - INFO: ### use_embedding_weight = True\n",
      "[2023-07-16 15:48:49] - INFO: ### batch_size = 16\n",
      "[2023-07-16 15:48:49] - INFO: ### max_sen_len = None\n",
      "[2023-07-16 15:48:49] - INFO: ### pad_index = 0\n",
      "[2023-07-16 15:48:49] - INFO: ### random_state = 2022\n",
      "[2023-07-16 15:48:49] - INFO: ### learning_rate = 4e-05\n",
      "[2023-07-16 15:48:49] - INFO: ### weight_decay = 0.1\n",
      "[2023-07-16 15:48:49] - INFO: ### masked_rate = 0.15\n",
      "[2023-07-16 15:48:49] - INFO: ### masked_token_rate = 0.8\n",
      "[2023-07-16 15:48:49] - INFO: ### masked_token_unchanged_rate = 0.5\n",
      "[2023-07-16 15:48:49] - INFO: ### log_level = 10\n",
      "[2023-07-16 15:48:49] - INFO: ### use_torch_multi_head = False\n",
      "[2023-07-16 15:48:49] - INFO: ### epochs = 200\n",
      "[2023-07-16 15:48:49] - INFO: ### model_val_per_epoch = 1\n",
      "[2023-07-16 15:48:49] - INFO: ### vocab_size = 21128\n",
      "[2023-07-16 15:48:49] - INFO: ### hidden_size = 768\n",
      "[2023-07-16 15:48:49] - INFO: ### num_hidden_layers = 12\n",
      "[2023-07-16 15:48:49] - INFO: ### num_attention_heads = 12\n",
      "[2023-07-16 15:48:49] - INFO: ### hidden_act = gelu\n",
      "[2023-07-16 15:48:49] - INFO: ### intermediate_size = 3072\n",
      "[2023-07-16 15:48:49] - INFO: ### pad_token_id = 0\n",
      "[2023-07-16 15:48:49] - INFO: ### hidden_dropout_prob = 0.1\n",
      "[2023-07-16 15:48:49] - INFO: ### attention_probs_dropout_prob = 0.1\n",
      "[2023-07-16 15:48:49] - INFO: ### max_position_embeddings = 512\n",
      "[2023-07-16 15:48:49] - INFO: ### type_vocab_size = 2\n",
      "[2023-07-16 15:48:49] - INFO: ### initializer_range = 0.02\n",
      "[2023-07-16 15:48:49] - INFO: ### directionality = bidi\n",
      "[2023-07-16 15:48:49] - INFO: ### pooler_fc_size = 768\n",
      "[2023-07-16 15:48:49] - INFO: ### pooler_num_attention_heads = 12\n",
      "[2023-07-16 15:48:49] - INFO: ### pooler_num_fc_layers = 3\n",
      "[2023-07-16 15:48:49] - INFO: ### pooler_size_per_head = 128\n",
      "[2023-07-16 15:48:49] - INFO: ### pooler_type = first_token_transform\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.embeddings.word_embeddings.weight赋值给bert_embeddings.word_embeddings.embedding.weight,参数形状为:torch.Size([21128, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.embeddings.position_embeddings.weight赋值给bert_embeddings.position_embeddings.embedding.weight,参数形状为:torch.Size([512, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.embeddings.token_type_embeddings.weight赋值给bert_embeddings.token_type_embeddings.embedding.weight,参数形状为:torch.Size([2, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.weight赋值给bert_embeddings.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.embeddings.LayerNorm.bias赋值给bert_embeddings.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.query.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.key.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.self.value.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.weight赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.intermediate.dense.bias赋值给bert_encoder.bert_layers.0.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.weight赋值给bert_encoder.bert_layers.0.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.dense.bias赋值给bert_encoder.bert_layers.0.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.weight赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.0.output.LayerNorm.bias赋值给bert_encoder.bert_layers.0.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.query.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.key.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.self.value.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.weight赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.intermediate.dense.bias赋值给bert_encoder.bert_layers.1.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.weight赋值给bert_encoder.bert_layers.1.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.dense.bias赋值给bert_encoder.bert_layers.1.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.weight赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.1.output.LayerNorm.bias赋值给bert_encoder.bert_layers.1.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.query.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.key.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.self.value.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.weight赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.intermediate.dense.bias赋值给bert_encoder.bert_layers.2.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.weight赋值给bert_encoder.bert_layers.2.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.dense.bias赋值给bert_encoder.bert_layers.2.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.weight赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.2.output.LayerNorm.bias赋值给bert_encoder.bert_layers.2.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.query.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.key.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.self.value.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.weight赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.intermediate.dense.bias赋值给bert_encoder.bert_layers.3.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.weight赋值给bert_encoder.bert_layers.3.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.dense.bias赋值给bert_encoder.bert_layers.3.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.weight赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.3.output.LayerNorm.bias赋值给bert_encoder.bert_layers.3.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.query.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.key.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.self.value.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.weight赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.intermediate.dense.bias赋值给bert_encoder.bert_layers.4.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.weight赋值给bert_encoder.bert_layers.4.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.dense.bias赋值给bert_encoder.bert_layers.4.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.weight赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.4.output.LayerNorm.bias赋值给bert_encoder.bert_layers.4.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.query.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.key.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.self.value.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.weight赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.intermediate.dense.bias赋值给bert_encoder.bert_layers.5.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.weight赋值给bert_encoder.bert_layers.5.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.dense.bias赋值给bert_encoder.bert_layers.5.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.weight赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.5.output.LayerNorm.bias赋值给bert_encoder.bert_layers.5.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.query.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.key.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.self.value.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.weight赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.intermediate.dense.bias赋值给bert_encoder.bert_layers.6.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.weight赋值给bert_encoder.bert_layers.6.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.dense.bias赋值给bert_encoder.bert_layers.6.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.weight赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.6.output.LayerNorm.bias赋值给bert_encoder.bert_layers.6.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.query.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.key.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.self.value.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.weight赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.intermediate.dense.bias赋值给bert_encoder.bert_layers.7.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.weight赋值给bert_encoder.bert_layers.7.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.dense.bias赋值给bert_encoder.bert_layers.7.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.weight赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.7.output.LayerNorm.bias赋值给bert_encoder.bert_layers.7.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.query.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.key.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.self.value.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.weight赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.intermediate.dense.bias赋值给bert_encoder.bert_layers.8.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.weight赋值给bert_encoder.bert_layers.8.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.dense.bias赋值给bert_encoder.bert_layers.8.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.weight赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.8.output.LayerNorm.bias赋值给bert_encoder.bert_layers.8.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.query.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.key.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.self.value.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.weight赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.intermediate.dense.bias赋值给bert_encoder.bert_layers.9.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.weight赋值给bert_encoder.bert_layers.9.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.dense.bias赋值给bert_encoder.bert_layers.9.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.weight赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.9.output.LayerNorm.bias赋值给bert_encoder.bert_layers.9.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.query.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.key.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.self.value.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.weight赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.intermediate.dense.bias赋值给bert_encoder.bert_layers.10.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.weight赋值给bert_encoder.bert_layers.10.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.dense.bias赋值给bert_encoder.bert_layers.10.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.weight赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.10.output.LayerNorm.bias赋值给bert_encoder.bert_layers.10.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.query.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.key.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.self.value.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.weight赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.attention.output.LayerNorm.bias赋值给bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.weight赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.weight,参数形状为:torch.Size([3072, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.intermediate.dense.bias赋值给bert_encoder.bert_layers.11.bert_intermediate.dense.bias,参数形状为:torch.Size([3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.weight赋值给bert_encoder.bert_layers.11.bert_output.dense.weight,参数形状为:torch.Size([768, 3072])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.dense.bias赋值给bert_encoder.bert_layers.11.bert_output.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.weight赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.weight,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.encoder.layer.11.output.LayerNorm.bias赋值给bert_encoder.bert_layers.11.bert_output.LayerNorm.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.pooler.dense.weight赋值给bert_pooler.dense.weight,参数形状为:torch.Size([768, 768])\n",
      "[2023-07-16 15:48:51] - DEBUG: ## 成功将参数:bert.pooler.dense.bias赋值给bert_pooler.dense.bias,参数形状为:torch.Size([768])\n",
      "[2023-07-16 15:48:51] - INFO: ## 注意，正在使用本地MyTransformer中的MyMultiHeadAttention实现，如需使用torch框架中的MultiHeadAttention模块可通过config.__dict__['use_torch_multi_head'] = True实现\n",
      "[2023-07-16 15:48:51] - INFO: ## 使用token embedding中的权重矩阵作为输出层的权重！torch.Size([21128, 768])\n",
      "[2023-07-16 15:48:51] - INFO: 缓存文件 /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi/songci_test_mlNone_rs2022_mr15_mtr8_mtur5.pt 不存在，重新处理并缓存！\n",
      "啦啦啦啦啦啦： /Users/czc/PycharmProjects/stockMarketAnalysis/bertTools/SongCi/songci.test.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "数据 model 不存在对应的格式化函数，请参考函数 read_wiki(filepath) 实现对应的格式化函数！",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m config \u001B[38;5;241m=\u001B[39m ModelConfig()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 103\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     89\u001B[0m bert_tokenize \u001B[38;5;241m=\u001B[39m BertTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(config\u001B[38;5;241m.\u001B[39mpretrained_model_dir)\u001B[38;5;241m.\u001B[39mtokenize\n\u001B[1;32m     90\u001B[0m data_loader \u001B[38;5;241m=\u001B[39m LoadBertPretrainingDataset(vocab_path\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mvocab_path,\n\u001B[1;32m     91\u001B[0m                                          tokenizer\u001B[38;5;241m=\u001B[39mbert_tokenize,\n\u001B[1;32m     92\u001B[0m                                          batch_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    100\u001B[0m                                          masked_token_rate\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmasked_token_rate,\n\u001B[1;32m    101\u001B[0m                                          masked_token_unchanged_rate\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mmasked_token_unchanged_rate)\n\u001B[1;32m    102\u001B[0m train_iter, test_iter, val_iter \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m--> 103\u001B[0m     \u001B[43mdata_loader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_train_val_test_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_file_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mtrain_file_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mval_file_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_file_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# Optimizer\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m# Split weights in two groups, one with weight decay and the other not.\u001B[39;00m\n\u001B[1;32m    108\u001B[0m no_decay \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayerNorm.weight\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/stockMarketAnalysis/bertTools/utils/create_pretraining_data.py:323\u001B[0m, in \u001B[0;36mLoadBertPretrainingDataset.load_train_val_test_data\u001B[0;34m(self, train_file_path, val_file_path, test_file_path, only_test)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_train_val_test_data\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    317\u001B[0m                              train_file_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    318\u001B[0m                              val_file_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    319\u001B[0m                              test_file_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    320\u001B[0m                              only_test\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    321\u001B[0m     postfix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_ml\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_sen_len\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_rs\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrandom_state\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_mr\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_rate)[\u001B[38;5;241m2\u001B[39m:]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \\\n\u001B[1;32m    322\u001B[0m               \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_mtr\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_token_rate)[\u001B[38;5;241m2\u001B[39m:]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_mtur\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_token_unchanged_rate)[\u001B[38;5;241m2\u001B[39m:]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 323\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_process\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_file_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mpostfix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpostfix\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    325\u001B[0m     test_iter \u001B[38;5;241m=\u001B[39m DataLoader(test_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m    326\u001B[0m                            shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_batch)\n\u001B[1;32m    327\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m only_test:\n",
      "File \u001B[0;32m~/PycharmProjects/stockMarketAnalysis/bertTools/utils/create_pretraining_data.py:82\u001B[0m, in \u001B[0;36mcache.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(data_path):\n\u001B[1;32m     81\u001B[0m     logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m缓存文件 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 不存在，重新处理并缓存！\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 82\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(data_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     84\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(data, f)\n",
      "File \u001B[0;32m~/PycharmProjects/stockMarketAnalysis/bertTools/utils/create_pretraining_data.py:237\u001B[0m, in \u001B[0;36mLoadBertPretrainingDataset.data_process\u001B[0;34m(self, filepath, postfix)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m本函数的作用是是根据格式化后的数据制作NSP和MLM两个任务对应的处理完成的数据\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m:param filepath:\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m啦啦啦啦啦啦：\u001B[39m\u001B[38;5;124m\"\u001B[39m,filepath)\n\u001B[0;32m--> 237\u001B[0m paragraphs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_format_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;66;03m# 返回的是一个二维列表，每个列表可以看做是一个段落（其中每个元素为一句话）\u001B[39;00m\n\u001B[1;32m    240\u001B[0m data \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/PycharmProjects/stockMarketAnalysis/bertTools/utils/create_pretraining_data.py:149\u001B[0m, in \u001B[0;36mLoadBertPretrainingDataset.get_format_data\u001B[0;34m(self, filepath)\u001B[0m\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m read_songci(filepath, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseps)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m数据 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 不存在对应的格式化函数，\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    150\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m请参考函数 read_wiki(filepath) 实现对应的格式化函数！\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: 数据 model 不存在对应的格式化函数，请参考函数 read_wiki(filepath) 实现对应的格式化函数！"
     ]
    }
   ],
   "source": [
    "config = ModelConfig()\n",
    "train(config)\n",
    "# sentences_1 = [\"I no longer love her, true, but perhaps I love her.\",\n",
    "#                \"Love is so short and oblivion so long.\"]\n",
    "# sentences_2 = [\"十年生死两茫茫。不思量。自难忘。千里孤坟，无处话凄凉。\",\n",
    "#                \"红酥手。黄藤酒。满园春色宫墙柳。\"]\n",
    "# inference(config, sentences_2, masked=False, language='zh',random_state=2022)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T07:48:52.578964Z",
     "start_time": "2023-07-16T07:48:49.271805Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.1 分类测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ### todo bert 情绪分析，从网上复制的，看看情况怎么样。 这一段的代码更多的是测试，需要提供文本预训练与最终分类训练结果,目前还没测试，后续准备好训练与预训练就开始\n",
    "\n",
    "# ### https://github.com/rsanshierli/EasyBert/tree/master/Sentiment\n",
    "# import re\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# from pytorch_pretrained import BertModel, BertTokenizer\n",
    "#\n",
    "#\n",
    "# class Config(object):\n",
    "#\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.model_name = 'bert'\n",
    "#         self.class_list = ['中性', '积极', '消极']          # 类别名单\n",
    "#         self.save_path = './Sentiment/saved_dict/bert.ckpt'        # 模型训练结果\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "#\n",
    "#         self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "#         self.num_classes = len(self.class_list)                         # 类别数\n",
    "#         self.num_epochs = 3                                             # epoch数\n",
    "#         self.batch_size = 128                                           # mini-batch大小\n",
    "#         self.pad_size = 32                                              # 每句话处理成的长度(短填长切)\n",
    "#         self.learning_rate = 5e-5                                       # 学习率\n",
    "#         self.bert_path = './bert_pretrain'\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "#         self.hidden_size = 768\n",
    "#\n",
    "#\n",
    "# class Model(nn.Module):\n",
    "#\n",
    "#     def __init__(self, config):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "#         for param in self.bert.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         context = x[0]  # 输入的句子\n",
    "#         mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "#         _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)\n",
    "#         out = self.fc(pooled)\n",
    "#         return out\n",
    "#\n",
    "#\n",
    "# PAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n",
    "#\n",
    "# def clean(text):\n",
    "#     # text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \" \", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "#     # text = re.sub(r\"\\[\\S+\\]\", \"\", text)  # 去除表情符号\n",
    "#     # text = re.sub(r\"#\\S+#\", \"\", text)  # 保留话题内容\n",
    "#     URL_REGEX = re.compile(\n",
    "#         r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "#         re.IGNORECASE)\n",
    "#     text = re.sub(URL_REGEX, \"\", text)  # 去除网址\n",
    "#     text = text.replace(\"转发微博\", \"\")  # 去除无意义的词语\n",
    "#     text = re.sub(r\"\\s+\", \" \", text)  # 合并正文中过多的空格\n",
    "#     return text.strip()\n",
    "#\n",
    "# def load_dataset(data, config):\n",
    "#     pad_size = config.pad_size\n",
    "#     contents = []\n",
    "#     for line in data:\n",
    "#         lin = clean(line)\n",
    "#         token = config.tokenizer.tokenize(lin)      # 分词\n",
    "#         token = [CLS] + token                           # 句首加入CLS\n",
    "#         seq_len = len(token)\n",
    "#         mask = []\n",
    "#         token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "#\n",
    "#         if pad_size:\n",
    "#             if len(token) < pad_size:\n",
    "#                 mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "#                 token_ids += ([0] * (pad_size - len(token)))\n",
    "#             else:\n",
    "#                 mask = [1] * pad_size\n",
    "#                 token_ids = token_ids[:pad_size]\n",
    "#                 seq_len = pad_size\n",
    "#         contents.append((token_ids, int(0), seq_len, mask))\n",
    "#     return contents\n",
    "#\n",
    "# class DatasetIterater(object):\n",
    "#     def __init__(self, batches, batch_size, device):\n",
    "#         self.batch_size = batch_size\n",
    "#         self.batches = batches     # data\n",
    "#         self.n_batches = len(batches) // batch_size\n",
    "#         self.residue = False  # 记录batch数量是否为整数\n",
    "#         if len(batches) % self.n_batches != 0:\n",
    "#             self.residue = True\n",
    "#         self.index = 0\n",
    "#         self.device = device\n",
    "#\n",
    "#     def _to_tensor(self, datas):\n",
    "#         x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "#         y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "#\n",
    "#         # pad前的长度(超过pad_size的设为pad_size)\n",
    "#         seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "#         mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "#         return (x, seq_len, mask), y\n",
    "#\n",
    "#     def __next__(self):     # 返回下一个迭代器对象，必须控制结束条件\n",
    "#         if self.residue and self.index == self.n_batches:\n",
    "#             batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "#             self.index += 1\n",
    "#             batches = self._to_tensor(batches)\n",
    "#             return batches\n",
    "#\n",
    "#         elif self.index >= self.n_batches:\n",
    "#             self.index = 0\n",
    "#             raise StopIteration\n",
    "#         else:\n",
    "#             batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "#             self.index += 1\n",
    "#             batches = self._to_tensor(batches)\n",
    "#             return batches\n",
    "#\n",
    "#     def __iter__(self):     # 返回一个特殊的迭代器对象，这个迭代器对象实现了 __next__() 方法并通过 StopIteration 异常标识迭代的完成。\n",
    "#         return self\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         if self.residue:\n",
    "#             return self.n_batches + 1\n",
    "#         else:\n",
    "#             return self.n_batches\n",
    "#\n",
    "#\n",
    "# def build_iterator(dataset, config):\n",
    "#     iter = DatasetIterater(dataset, 1, config.device)\n",
    "#     return iter\n",
    "#\n",
    "#\n",
    "# def match_label(pred, config):\n",
    "#     label_list = config.class_list\n",
    "#     return label_list[pred]\n",
    "#\n",
    "#\n",
    "# def final_predict(config, model, data_iter):\n",
    "#     map_location = lambda storage, loc: storage\n",
    "#     model.load_state_dict(torch.load(config.save_path, map_location=map_location))\n",
    "#     model.eval()\n",
    "#     predict_all = np.array([])\n",
    "#     with torch.no_grad():\n",
    "#         for texts, _ in data_iter:\n",
    "#             outputs = model(texts)\n",
    "#             pred = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "#             pred_label = [match_label(i, config) for i in pred]\n",
    "#             predict_all = np.append(predict_all, pred_label)\n",
    "#\n",
    "#     return predict_all\n",
    "#\n",
    "# def main(text):\n",
    "#     config = Config()\n",
    "#     model = Model(config).to(config.device)\n",
    "#     test_data = load_dataset(text, config)\n",
    "#     test_iter = build_iterator(test_data, config)\n",
    "#     result = final_predict(config, model, test_iter)\n",
    "#     for i, j in enumerate(result):\n",
    "#         print('text:{}'.format(text[i]))\n",
    "#         print('label:{}'.format(j))\n",
    "#\n",
    "#\n",
    "# if __name__ == '__main__':\n",
    "#\n",
    "#     test = ['#你好2020#新年第一天元气满满的早起出门买早饭结果高估了自己抗冻能力回家成功冻发烧（大概是想告诉我2020要量力而行）然鹅这并不影响后续计划一出门立马生龙活虎新年和新??更配哦??看了误杀吃了大餐就让新的一年一直这样美滋滋下去吧??',\n",
    "#             '大宝又感冒鼻塞咳嗽了，还有发烧。队友加班几天不回。感觉自己的情绪在家已然是随时引爆的状态。情绪一上来，容易对孩子说出自己都想不到的话来……2020年，真的要学会控制情绪，管理好家人健康。这是今年最大的目标。?',\n",
    "#             '还要去输两天液，这天也太容易感冒发烧了，一定要多喝热水啊?',\n",
    "#             '我太难了别人怎么发烧都没事就我一检查甲型流感?',\n",
    "#             '果然是要病一场的喽回来第三天开始感冒今儿还发烧了喉咙眼睛都难受的一匹怎么样能不经意让我的毕设导师看到这条微博并给我放一天假呢?']\n",
    "#     main(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-16T07:48:52.567987Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
